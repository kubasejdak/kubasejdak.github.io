{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"about/","title":"About Me","text":"<p>Hi! My name is Kuba: I do designing and programming in C++ for a living. After hours I admire its beauty and work on projects in both high-level and embedded domains.</p> <p></p>"},{"location":"about/#bio","title":"Bio","text":"<p>I started coding before the high school. From the first days I knew, that C++ will play a tremendous role in my carrier. After 12 years nothing changed. In my professional work I had a chance to participate in projects from various sectors:</p> <ul> <li>industrial factories,</li> <li>smart TVs,</li> <li>PLC (Power Line Communication) networks,</li> <li>energy meters,</li> <li>gas meters,</li> <li>proprietary operating systems,</li> <li>UI applications for marine electronics,</li> <li>plasma power supplies.</li> </ul> <p>Some of them followed the best programming practices, but the others were just unmaintainable pieces of crap. Also, project management approaches varied from democracy, through constitutional monarchy up to tyranny. All those experiences shaped me as a mature software engineer capable of handling projects until the happy end.</p> <p>C++ has entered a phase of a rapid evolution and has become exciting as never before. Ben Parker from Spider-Man once said:</p> <pre><code>\"With great power comes great responsibility.\"\n</code></pre> <p>This statement is especially important in post-C++98 era. When I started this blog I had an ambition to share all my observations and tips, that I learned in my daily work. I hope that it will help both me and you in being a better programmer.</p> <p>If you have any questions, concerns or proposals for topics to write about \u2013 contact me directly. I will be thrilled to learn something new and share that with you in simple words.</p>"},{"location":"about/#resume","title":"Resume","text":"<p>Below you can find a brief summary of my professional life. You can also download it as a PDF here.</p> COMPANY POSITION PERIOD TRUMPF Huettinger Software Architect 07.2023 - now TRUMPF Huettinger Senior Software Engineer 07.2022 - 06.2023 TRUMPF Huettinger Software Engineer 01.2019 - 06.2022 Polish C++ Standardization Body Technical Expert Member 09.2019 - 03.2022 Mobica Senior Software Engineer 09.2017 - 12.2018 Phoenix Systems Operating System Engineer 05.2015 - 07.2017 Samsung Electronics R&amp;D Software Engineer 03.2014 - 04.2015 Samsung Electronics R&amp;D Junior Software Engineer 06.2012 - 02.2014 RRW Automatic Freelancer 12.2011 Samsung Electronics R&amp;D Internship 07.2011 - 09.2011 RRW Automatic Freelancer 04.2011"},{"location":"about/#commercial-work","title":"Commercial work","text":""},{"location":"about/#polish-c-standardization-body-technical-expert-member","title":"Polish C++ Standardization Body (Technical Expert Member)","text":"<ul> <li>Reviewing, giving opinion and participating in the development of the ISO C++ standard.</li> </ul>"},{"location":"about/#trumpf-huettinger-software-architect","title":"TRUMPF Huettinger (Software Architect)","text":"<p>TODO</p>"},{"location":"about/#trumpf-huettinger-senior-software-engineer","title":"TRUMPF Huettinger (Senior Software Engineer)","text":"<p>TODO</p>"},{"location":"about/#trumpf-huettinger-software-engineer","title":"TRUMPF Huettinger (Software Engineer)","text":"<ul> <li>Design and implementation of a flexible and robust hardware abstraction layer (HAL) for plasma power supplies.</li> <li>Implementation and porting of various device drivers for TI SYS/BIOS and PSDK platforms.</li> <li>Maintenance of coding rules and standards for C++.</li> </ul>"},{"location":"about/#mobica-senior-software-engineer","title":"Mobica (Senior Software Engineer)","text":"<ul> <li>Project leader in the core UI feature development for the leading marine electronics vendor.</li> <li>Implementation of a robust QML application for displaying gauges/indicators on the marine MFD.</li> <li>Gathering requirements and use cases.</li> <li>Reverse engineering of a complex embedded firmware in the fire-safety industry.</li> <li>Creating full top to bottom architecture of the analyzed firmware in UML.</li> </ul>"},{"location":"about/#phoenix-systems-operating-system-engineer","title":"Phoenix Systems (Operating System Engineer)","text":"<ul> <li>Project leader in the commercialization of the Phoenix-RTOS for the gas meters industry.</li> <li>Design and implementation of a real-time operating system (Phoenix-RTOS) for the embedded devices (full user-kernel   space and user-space separation with MMU or MPU).</li> <li>Porting POSIX functionality to the Phoenix-RTOS.</li> <li>Custom implementation of the DNS and SNMP stacks (client and server).</li> <li>Porting GCC compiler and Newlib (libc) for the Phoenix-RTOS in official GNU repositories.</li> <li>Design and implementation of the GUI tools for inspecting the PLC (Power Line Communication) networks in a Smart Grid   industry.</li> </ul>"},{"location":"about/#samsung-electronics-rd-software-engineer","title":"Samsung Electronics R&amp;D (Software Engineer)","text":"<ul> <li> Award (2015): Bonus award for the hard work and participation in the Tizen TV project.</li> <li> Award (2014): Highest performance appraisal in the part (~ 20 people) at the end of year.</li> <li> Award (2014): Best employee in the group (~ 80 people).</li> <li>Design and implementation of multiple modules in the SMART TV software upgrade.</li> <li>Integrating software upgrade with the Tizen operating system (embedded Linux).</li> <li>Leader of the internal project with 2 trainees.</li> <li>Leader of the software upgrade component with up to 4-5 programmers.</li> </ul>"},{"location":"about/#samsung-electronics-rd-junior-software-engineer","title":"Samsung Electronics R&amp;D (Junior Software Engineer)","text":"<ul> <li> Award (2013): Highest performance appraisal in the part (~20 people) at the end of the year.</li> <li> Award (2013): SRPOL 2013 4Q Award.</li> <li>Implementation of the DVB and ISDB analysis tools.</li> <li>Design and implementation of the OTA upgrade transportation protocol for the SMART TV.</li> <li>SMART TV software upgrade development: OTN, OTA, USB.</li> </ul>"},{"location":"about/#rrw-automatic-freelancer","title":"RRW Automatic (Freelancer)","text":"<ul> <li>Implementation of the application for configuration of a 3D camera in the Smurfit Kappa factory (dashboards).</li> <li>Training for the users.</li> </ul>"},{"location":"about/#samsung-electronics-poland-rd-internship","title":"Samsung Electronics Poland R&amp;D (Internship)","text":"<ul> <li>Implementation of a module for analysis and modification of the MPEG-4 DVB streams on the fly.</li> <li>Design of an algorithm to optimize modulator assignments for the users.</li> <li>Implementation of a web administration tool.</li> </ul>"},{"location":"about/#rrw-automatic-freelancer_1","title":"RRW Automatic (Freelancer)","text":"<ul> <li>Implementation of the application co-operating with a vision system in the Nycomed factory (pharmacy) and controlling   several production line mechanisms.</li> <li>Training for the users.</li> </ul>"},{"location":"about/#education","title":"Education","text":""},{"location":"about/#warsaw-university-of-technology","title":"Warsaw University of Technology","text":"<ul> <li>Faculty of Electronics and Information Technology \u2013 Computer Science</li> <li> Engineer\u2019s Thesis: Shellcode similarity analysis by means of its execution graph.</li> <li> Masters\u2019s Thesis: Methodology of hiding malicious code from antivirus engines.</li> <li> Award (2013): Graduation with distinction for the Engineer\u2019s Thesis and very good marks.</li> <li> Award (2014): Graduation with distinction for the Master\u2019s Thesis and very good marks.</li> </ul>"},{"location":"about/#xiv-high-school-im-stanisawa-staszica-in-warsaw","title":"XIV High School im. Stanis\u0142awa Staszica in Warsaw","text":"<ul> <li>Class with the special program in mathematics and computer science.</li> <li>The best high school is Warsaw and Poland according to the Perspektywy ranking.</li> </ul>"},{"location":"about/#private-life","title":"Private life","text":"<p>You can\u2019t just live to work! Besides computers and technologies I am a big fan of Star Wars and Marvel movies. The first one are accompanied by an enormous expanded universe world described in nearly 200 books that I collect and read one-by-one \ud83d\ude42 If I really have to go out, then I run or ride my Yamaha FZ6 Fazer around the Poland.</p> <p></p>"},{"location":"services/","title":"Services","text":"<p>Thanks for trusting me. I believe in working in a way, that I won\u2019t have to be ashamed of it in the future. I will make every effort to meet all your expectations. Don\u2019t hesitate to share your coding problems and needs with me. If I can\u2019t help you, then I will point you to someone who can and I trust.</p>"},{"location":"services/#offer","title":"Offer","text":"<p>I am available for hire in the following domains:</p> <ul> <li>software development (big &amp; small projects):</li> <li>UI applications (C/C++, Qt, UML),</li> <li>embedded systems,</li> <li>backend,</li> <li>architecture and design,</li> <li>refactoring and modernizing,</li> <li>project feasibility and estimation,</li> <li>training,</li> <li>consulting,</li> <li>project management,</li> <li>technology selection,</li> <li>establishing/enforcing coding standards,</li> <li>code reviews,</li> <li>static analysis.</li> </ul>"},{"location":"services/#contracts","title":"Contracts","text":"<p>I am open to both time &amp; material and fixed-price contracts. However, the preferred choice is dependent on individual needs and constraints. The rule of thumb is: the more is known in advance and relies only on me, the more I\u2019m for the fixed-price projects.</p>"},{"location":"services/#location","title":"Location","text":"<p>Currently I\u2019m unable to move permanently outside Poland. This is why my relocation and traveling is very limited. My preference is to work 100% remotely. On the other hand I understand how important it is to meet in person, get to know each other and discuss project details before the actual work begins. If needed, I can arrange:</p> <ul> <li>kick-off meeting (at client site or in arbitrary location),</li> <li>remote meeting/conference call,</li> <li>periodic meetings.</li> </ul>"},{"location":"services/#contact","title":"Contact","text":"<p>If you have any questions or you want to discuss the business possibilities, just send me a short notice. I will respond as quick as possible. Looking forward to hearing from you!</p>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#tag:cmake","title":"CMake","text":"<ul> <li>            19 reasons why CMake is actually awesome          </li> <li>            How to cross-compile for embedded with CMake like a champ          </li> <li>            How to join repositories in CMake          </li> <li>            Modern CMake is like inheritance          </li> </ul>"},{"location":"tags/#tag:conan","title":"Conan","text":"<ul> <li>            Introduction to Conan package manager          </li> <li>            Problems with external dependencies in C++          </li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/19-reasons-why-cmake-is-actually-awesome/","title":"19 reasons why CMake is actually awesome","text":"<p>Topic of CMake is extremely controversial in the C/C++ community. People say that it is hard to properly set the include paths, that syntax is archaic or that managing dependencies is a nightmare. Expressing public hate for CMake has become a way of integrating with other software developers on the Internet.</p> <p>And I partially understand that statements. Before CMake 3.x (aka Modern CMake) we were forced to use <code>include_directories()</code> or manually set installation paths for libraries, that were not supported by default in CMake. Syntax was a bit oldish and managing the compilation flags usually lead to headaches.</p> <p>But since CMake 3.x (released on 06.2014) we have way more flexible and elegant ways of creating build systems with CMake. Almost all cons are gone in favor of the new modern solutions. The only problem is that once a bad impression was made, it is really hard to change it. After all, who would like to invest in studying new features and best practices of a build system?! Well I do, and let me share with you 19 reasons, why CMake is actually awesome and you should give it a try again.</p> <p>Info</p> <p>For someone who is completely new to the topic: CMake is a build system generator, that creates build system files for common frameworks and IDE\u2019s from a generic scripting language.</p> <p>Disclaimer</p> <p>This article aims to give a brief overview of CMake capabilities without going deep into technical details. Consult the official CMake docs for more details.</p>","tags":["CMake"]},{"location":"blog/19-reasons-why-cmake-is-actually-awesome/#1-cmake-is-cross-platform","title":"1. CMake is cross-platform","text":"<p>This one should be obvious since CMake has been cross-platform from the beginning. Currently it can run on all major platforms:</p> <ul> <li>Windows,</li> <li>Linux,</li> <li>MacOS.</li> </ul> <p>It means, that we can build projects which use CMake on all above platforms without additional platform-specific configurations. No need to write Makefiles, configure Visual Studio projects, create custom Bash or batch files. Everything is handled by CMake. Note, that I say \u201cbuild on\u201d, not \u201cbuild for\u201d. There is a clear separation between build host and build target. Of course case when host and target is the same is assumed by default.</p>","tags":["CMake"]},{"location":"blog/19-reasons-why-cmake-is-actually-awesome/#2-cmake-supports-multiple-ides-and-build-frameworks","title":"2. CMake supports multiple IDE\u2019s and build frameworks","text":"<p>CMake is a build system generator (meta build system), which means that it creates configuration files for other existing build systems. List of available options is dependent on the build host, which is natural (usually there is no need to generate Visual Studio projects on Linux).</p> <p>Below you can find a complete list (as for publication date) of all supported frameworks (CMake is calling them generators):</p> <ul> <li>Borland Makefiles,</li> <li>MSYS Makefiles,</li> <li>MinGW Makefiles,</li> <li>NMake Makefiles,</li> <li>NMake Makefiles JOM,</li> <li>Unix Makefiles,</li> <li>Watcom WMake,</li> <li>Ninja,</li> <li>Visual Studio 6,</li> <li>Visual Studio 7,</li> <li>Visual Studio 7 .NET 2003,</li> <li>Visual Studio 8 2005,</li> <li>Visual Studio 9 2008,</li> <li>Visual Studio 10 2010,</li> <li>Visual Studio 11 2012,</li> <li>Visual Studio 12 2013,</li> <li>Visual Studio 14 2015,</li> <li>Visual Studio 15 2017,</li> <li>Visual Studio 16 2019,</li> <li>Green Hills MULTI,</li> <li>Xcode,</li> <li>CodeBlocks,</li> <li>CodeLite,</li> <li>Eclipse CDT4,</li> <li>Kate,</li> <li>Sublime Text 2.</li> </ul> <p>Only by using different command line option you can target either MacOS XCode or Windows Visual Studio. Quite impressive, wouldn\u2019t you say?</p> <ol> <li> <p>CMake supports native shell commands/apps execution Despite being OS-independent, CMake still gives you ability to execute custom host-specific shell commands or launch given application with the following commands:</p> </li> <li> <p><code>execute_process()</code>,</p> </li> <li><code>add_custom_command()</code>.</li> </ol> <p>However, this is not a good practice because it makes our build system tightly coupled with the concrete platform (which is in contrary to the purpose of CMake).</p>","tags":["CMake"]},{"location":"blog/19-reasons-why-cmake-is-actually-awesome/#4-cmake-allows-easy-external-project-download-and-incorporation","title":"4. CMake allows easy external project download and incorporation","text":"<p>In some cases we are forced to use some external projects from the remote location (e.g. Internet) or simply from another local repository/disk directory. CMake has two integrated mechanisms for supporting that:</p> <ul> <li><code>FetchContent()</code>,</li> <li><code>ExternalProject_Add()</code>.</li> </ul> <p>Both commands are almost identical except for one crucial difference: <code>FetchContent</code> is launched during the generation time (when you call cmake) and <code>ExternalProject_Add</code> is launched during build time (e.g. when you call make).</p> <p>Its usage is straightforward \u2013 you have to specify Git/SVN/file/other location and optional parameters (e.g. repository branch) and CMake will automatically download and optionally build that project for you.</p> <p>Here you have an example of downloading <code>libfmt</code> project from GitHub using <code>v11.0.2</code> tag:</p> <pre><code>include(FetchContent)\nFetchContent_Declare(fmt\n    GIT_REPOSITORY https://github.com/fmtlib/fmt.git\n    GIT_TAG        v11.0.2\n    SYSTEM\n)\n\nFetchContent_MakeAvailable(fmt)\n</code></pre> <p>After that, you have <code>libfmt</code> downloaded in your build directory and added to the project. Now you can treat that it like normal source path (in particular link with any targets defined by <code>libfmt</code> repo).</p>","tags":["CMake"]},{"location":"blog/19-reasons-why-cmake-is-actually-awesome/#5-cmake-supports-cross-compilation-like-a-champ","title":"5. CMake supports cross-compilation like a champ","text":"<p>Cross-compilation (building for platform other than you use for compilation) has always scared me. A task nearly impossible to do manually in a cross-platform manner.</p> <p>CMake has this ability built-in and requires very little effort to make it work for you. All you need to do is to provide the so called toolchain file. It is a normal CMake file, except it doesn\u2019t require its name to be <code>CMakeLists.txt</code> (actually I would prefer it to be called <code>&lt;toolchain_name&gt;.cmake</code>). Below you can see an example toolchain file for arm-none-eabi-gcc toolchain (<code>arm-none-eabi-gcc.cmake</code>):</p> <pre><code>set(CMAKE_SYSTEM_NAME               Generic)\nset(CMAKE_SYSTEM_PROCESSOR          arm)\n\n# Without that flag CMake is not able to pass test compilation check\nset(CMAKE_TRY_COMPILE_TARGET_TYPE   STATIC_LIBRARY)\n\nset(CMAKE_AR                        ${BAREMETAL_ARM_TOOLCHAIN_PATH}/bin/arm-none-eabi-ar)\nset(CMAKE_ASM_COMPILER              ${BAREMETAL_ARM_TOOLCHAIN_PATH}/bin/arm-none-eabi-gcc)\nset(CMAKE_C_COMPILER                ${BAREMETAL_ARM_TOOLCHAIN_PATH}/bin/arm-none-eabi-gcc)\nset(CMAKE_CXX_COMPILER              ${BAREMETAL_ARM_TOOLCHAIN_PATH}/bin/arm-none-eabi-g++)\nset(CMAKE_LINKER                    ${BAREMETAL_ARM_TOOLCHAIN_PATH}/bin/arm-none-eabi-ld)\nset(CMAKE_OBJCOPY                   ${BAREMETAL_ARM_TOOLCHAIN_PATH}/bin/arm-none-eabi-objcopy CACHE INTERNAL \"\")\nset(CMAKE_RANLIB                    ${BAREMETAL_ARM_TOOLCHAIN_PATH}/bin/arm-none-eabi-ranlib CACHE INTERNAL \"\")\nset(CMAKE_SIZE                      ${BAREMETAL_ARM_TOOLCHAIN_PATH}/bin/arm-none-eabi-size CACHE INTERNAL \"\")\nset(CMAKE_STRIP                     ${BAREMETAL_ARM_TOOLCHAIN_PATH}/bin/arm-none-eabi-strip CACHE INTERNAL \"\")\nset(CMAKE_GCOV                      ${BAREMETAL_ARM_TOOLCHAIN_PATH}/bin/arm-none-eabi-gcov CACHE INTERNAL \"\")\n\nset(COMMON_FLAGS                    \"-Wno-psabi --specs=nosys.specs -fdata-sections -ffunction-sections -Wl,--gc-sections\")\nset(CMAKE_C_FLAGS                   \"${COMMON_FLAGS} ${APP_C_FLAGS} \" CACHE INTERNAL \"\")\nset(CMAKE_CXX_FLAGS                 \"${COMMON_FLAGS} -fno-exceptions ${APP_CXX_FLAGS}\" CACHE INTERNAL \"\")\n\nset(CMAKE_C_FLAGS_DEBUG             \"-Os -g\" CACHE INTERNAL \"\")\nset(CMAKE_C_FLAGS_RELEASE           \"-Os -DNDEBUG\" CACHE INTERNAL \"\")\nset(CMAKE_CXX_FLAGS_DEBUG           \"${CMAKE_C_FLAGS_DEBUG}\" CACHE INTERNAL \"\")\nset(CMAKE_CXX_FLAGS_RELEASE         \"${CMAKE_C_FLAGS_RELEASE}\" CACHE INTERNAL \"\")\n\nset(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM NEVER)\nset(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY)\nset(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE ONLY)\n</code></pre> <p>As you can see, this file only sets paths to several tools from the toolchain and sets common compilation flags (specific for that compiler).</p> <p>In order to tell CMake to use this configuration you have to set the <code>CMAKE_TOOLCHAIN_FILE</code> variable before the call to <code>project()</code> function. So you can either set it as a command line argument:</p> <pre><code>cmake &lt;PATH_TO_SOURCES&gt; -DCMAKE_TOOLCHAIN_FILE=&lt;PATH_TO_TOOLCHAIN_FILE&gt;\n</code></pre> <p>or as a normal variable in the root <code>CMakeLists.txt</code>:</p> <pre><code>cmake_minimum_required(VERSION 3.15)\n\nset(CMAKE_TOOLCHAIN_FILE &lt;PATH_TO_TOOLCHAIN_FILE&gt;)\n\nproject(&lt;PROJECT_NAME&gt; ...)\n\n...\n</code></pre>","tags":["CMake"]},{"location":"blog/19-reasons-why-cmake-is-actually-awesome/#6-cmake-makes-it-easy-to-handle-include-paths","title":"6. CMake makes it easy to handle include paths","text":"<p>Before CMake 3.x when you wanted to set the include paths for the current directory and everything below you had to write:</p> <pre><code>include_directories(&lt;PATH_TO_INCLUDES&gt;)\n</code></pre> <p>This is bad! Don\u2019t do that unless you know what you are doing. This is an anti-pattern of software architecture. In order to get the includes from another library you had to know its internal structure (to include the proper sets of paths). And what if that library is changed? You have to adjust every time. Nightmare!</p> <p>Wouldn\u2019t it be nice if that library provided you with everything you need with one statement? Since modern CMake (3.x+) it is possible! Library author can utilize the fact, that when you \u201clink\u201d with that library via <code>target_link_libraries</code> (quite unfortunate name: read this for a reason why) you \u201cinherit\u201d all its public properties. And the include paths are the property of the library. They can set via <code>target_include_directories</code> function.</p> <p>So in other words, the library sets its include paths as its \u201cinclude\u201d property and when you link with it you automatically get them. No need to write anything manually. Also the internal structure of the library is completely hidden:</p> <code>CMakeLists.txt</code> for library A <code>CMakeLists.txt</code> for library B <pre><code>add_library(A &lt;LIBRARY_A_SOURCES&gt;)\n\ntarget_include_directories(\n    PUBLIC liba/include\n)\n</code></pre> <pre><code>add_library(B &lt;LIBRARY_B_SOURCES&gt;)\n\ntarget_link_libraries(B\n    PRIVATE A             # Here we automatically get the liba/include includes.\n)\n</code></pre> <p>Note</p> <p><code>PUBLIC</code>, <code>PRIVATE</code> and <code>INTERFACE</code> specifiers are explained in this article.</p>","tags":["CMake"]},{"location":"blog/19-reasons-why-cmake-is-actually-awesome/#7-cmake-makes-it-easy-to-handle-compilation-linking-and-preprocessor-flags","title":"7. CMake makes it easy to handle compilation, linking and preprocessor flags","text":"<p>This paragraph is similar to the previous one. All the problems with the include paths are valid also for compilation, linking and preprocessor flags. And we also want other libraries to provide all necessary flags to use them. CMake allows that with the following functions:</p> <ul> <li><code>target_compile_definitions(&lt;PREPROCESSOR_FLAGS&gt;)</code>,</li> <li><code>target_compile_options(&lt;COMPILATION_FLAGS&gt;)</code>,</li> <li><code>target_link_options(&lt;LINKER_FLAGS&gt;)</code>.</li> </ul> <code>CMakeLists.txt</code> for library A <code>CMakeLists.txt</code> for library B <pre><code>add_library(A &lt;LIBRARY_A_SOURCES&gt;)\n\ntarget_compile_definitions(\n    PUBLIC -DUSE_TIMER -DNO_EXCEPTIONS\n)\n\ntarget_compile_options(\n    PUBLIC \u2014std=c++17\n)\n\ntarget_link_options(\n    PUBLIC -T linker_script.ld\n)\n</code></pre> <pre><code>add_library(B &lt;LIBRARY_B_SOURCES&gt;)\n\ntarget_link_libraries(B\n    PRIVATE A             # Here we automatically get the -DUSE_TIMER -DNO_EXCEPTIONS \u2014std=c++17 -T linker_script.ld flags.\n)\n</code></pre>","tags":["CMake"]},{"location":"blog/19-reasons-why-cmake-is-actually-awesome/#8-cmake-is-supported-by-all-major-c-package-managers","title":"8. CMake is supported by all major C++ package managers","text":"<p>There are 3 major package managers for C++:</p> <ul> <li>Conan,</li> <li>vcpkg,</li> <li>Hunter.</li> </ul> <p>All of them have built-in native support for CMake. This article is already too long to show examples for each manager, so you have to trust me on this. Each of them is easy to use and doesn\u2019t make a mess in your carefully crafted <code>CMakeLists.txt</code> files.</p>","tags":["CMake"]},{"location":"blog/19-reasons-why-cmake-is-actually-awesome/#9-cmake-is-backward-compatible","title":"9. CMake is backward compatible","text":"<p>CMake is an actively developed project and new versions are released quite often. Some of them add new features and some slightly modify existing ones. Within the development team, people may be using different operating systems and different versions of that systems. It is very likely that there will be at least few different versions of CMake in use among project members. This could get messy if someone uses feature that is not available for everyone. Also bugs in CMake may be different for everyone.</p> <p>In order to tame the chaos CMake introduced policies, which strictly define how particular CMake feature should behave. Every CMake version has a set of its default policies and is able to load those sets for every previous version. So for example despite having CMake 3.15 we can enforce the binary to behave like version 3.7.</p> <p>We can do that by specifying the following statement:</p> <pre><code>cmake_minimum_required(VERSION &lt;CMAKE_VERSION_TO_BE_USED&gt;)\n</code></pre> <p>It is mandatory to set CMake version in the at the beginning of the root <code>CMakeLists.txt</code>, but we can change that later in any place (however I strongly advice against that!).</p>","tags":["CMake"]},{"location":"blog/19-reasons-why-cmake-is-actually-awesome/#10-cmake-is-supported-by-many-ides","title":"10. CMake is supported by many IDEs","text":"<p>By this I mean, that many IDEs can use existing CMakeLists.txt directly as a project file. All you have to do is point to the root CMakeLists.txt.</p> <p>Here is a list of the most popular ones:</p> <ul> <li>CLion,</li> <li>Visual Studio Code,</li> <li>QtCreator,</li> <li>Visual Studio,</li> <li>NetBeans,</li> <li>KDevelop.</li> </ul>","tags":["CMake"]},{"location":"blog/19-reasons-why-cmake-is-actually-awesome/#11-cmake-allows-basic-file-manipulation","title":"11. CMake allows basic file manipulation","text":"<p>Sometimes during configure or build time we need to interact with our filesystem. It usually is a simple operation like reading contents of the file (e.g. containing compilation flags) or copying (e.g. from resource directory to the build directory).</p> <p>CMake offers basic file manipulation functions, just like in the system console. Below you can find an extract from the official CMake file commands documentation:</p> <pre><code>Reading\n  file(READ &lt;filename&gt; &lt;out-var&gt; [...])\n  file(STRINGS &lt;filename&gt; &lt;out-var&gt; [...])\n  file(&lt;HASH&gt; &lt;filename&gt; &lt;out-var&gt;)\n  file(TIMESTAMP &lt;filename&gt; &lt;out-var&gt; [...])\n  file(GET_RUNTIME_DEPENDENCIES [...])\n\nWriting\n  file({WRITE | APPEND} &lt;filename&gt; &lt;content&gt;...)\n  file({TOUCH | TOUCH_NOCREATE} [&lt;file&gt;...])\n  file(GENERATE OUTPUT &lt;output-file&gt; [...])\n\nFilesystem\n  file({GLOB | GLOB_RECURSE} &lt;out-var&gt; [...] [&lt;globbing-expr&gt;...])\n  file(RENAME &lt;oldname&gt; &lt;newname&gt;)\n  file({REMOVE | REMOVE_RECURSE } [&lt;files&gt;...])\n  file(MAKE_DIRECTORY [&lt;dir&gt;...])\n  file({COPY | INSTALL} &lt;file&gt;... DESTINATION &lt;dir&gt; [...])\n  file(SIZE &lt;filename&gt; &lt;out-var&gt;)\n  file(READ_SYMLINK &lt;linkname&gt; &lt;out-var&gt;)\n  file(CREATE_LINK &lt;original&gt; &lt;linkname&gt; [...])\n\nPath Conversion\n  file(RELATIVE_PATH &lt;out-var&gt; &lt;directory&gt; &lt;file&gt;)\n  file({TO_CMAKE_PATH | TO_NATIVE_PATH} &lt;path&gt; &lt;out-var&gt;)\n\nTransfer\n  file(DOWNLOAD &lt;url&gt; &lt;file&gt; [...])\n  file(UPLOAD &lt;file&gt; &lt;url&gt; [...])\n\nLocking\n  file(LOCK &lt;path&gt; [...])\n</code></pre> <p>All of of this in a cross-platform manner.</p>","tags":["CMake"]},{"location":"blog/19-reasons-why-cmake-is-actually-awesome/#12-cmake-offers-similar-functionality-as-system-shell","title":"12. CMake offers similar functionality as system shell","text":"<p>Most of the operations that you would like to perform on variables in Bash or any other shell along with basic flow control and code organization can be done in CMake as well. We are talking here about:</p> <ul> <li>setting/getting value of the variable,</li> <li>loops,</li> <li>functions and macros,</li> <li>lists manipulation,</li> <li>conditional branches (<code>if</code>/<code>else if</code>/<code>else</code>),</li> <li>setting/unsetting environmental variables,</li> <li>and much more.</li> </ul>","tags":["CMake"]},{"location":"blog/19-reasons-why-cmake-is-actually-awesome/#13-cmake-can-be-launched-with-gui","title":"13. CMake can be launched with GUI","text":"<p>Yes, CMake comes with a full-featured GUI (however it may require installation of an additional package). Consult the official CMake GUI docs.</p>","tags":["CMake"]},{"location":"blog/19-reasons-why-cmake-is-actually-awesome/#14-cmake-has-ability-to-locate-installed-libraries-in-the-system","title":"14. CMake has ability to locate installed libraries in the system","text":"<p>Dependency management can be really painful when it comes to handling external libraries. Especially, if we have to rely on their presence in the system. Each OS or even package can have a different default installation path.</p> <p>CMake has a built-in function to locate a predefined set of packages by name and automatically set variables with include paths, sources or even expose whole \u201cmodern\u201d target which has it all set as properties:</p> <pre><code>find_package(Boost 1.56 REQUIRED COMPONENTS date_time filesystem iostreams)\n\nadd_executable(foo foo.cpp)\n\ntarget_link_libraries(foo PRIVATE Boost::date_time Boost::filesystem Boost::iostreams)\n</code></pre> <p>The list of supported libraries is too long (as of CMake 3.16) to put it here, but feel free to check it in the official docs.</p> <p>Note</p> <p>You can also specify an additional location with your own packages finder files.</p>","tags":["CMake"]},{"location":"blog/19-reasons-why-cmake-is-actually-awesome/#15-cmake-allows-creating-static-shared-and-header-only-libraries","title":"15. CMake allows creating static, shared and header-only libraries","text":"<p>You can explicitly tell CMake to create a static (default) or shared library by adding an extra parameter to <code>add_library</code>:</p> <pre><code>add_library(&lt;NAME&gt; [SHARED | STATIC] &lt;SOURCES&gt;)\n</code></pre> <p>Header-only libraries are a bit less obvious to create, but still quite easy. All you need to do is to:</p> <ul> <li>specify that given library is an interface (no binary is produced): <code>add_library(&lt;NAME&gt; INTERFACE)</code>,</li> <li>set its interface include path to the location of your headers: <code>target_include_directories(&lt;NAME&gt; INTERFACE .)</code>.</li> </ul>","tags":["CMake"]},{"location":"blog/19-reasons-why-cmake-is-actually-awesome/#16-cmake-allows-codefiles-generation","title":"16. CMake allows code/files generation","text":"<p>Sometimes we want to add some compile-time information to our sources, that will be automatically computed or generated. An excellent example would be a string with the project version.</p> <p>One way of having that would be to change manually this value directly in the header. And it is OK. But what if this value has to be used in multiple contexts, some of which are not inside the source code? For example in documentation, in some build artifact etc. You may also want to extract this value from git tag. What then?</p> <p>Fortunately CMake has a generic way of creating new files from a predefined template. All you need to do is to create a file that you want to be generated and replace all its \u201cvariable\u201d parts with the CMake variable notation.</p> <p>Then you can call <code>configure_file()</code> command and CMake will create a copy of that template and replace all \u201cvariable\u201d parts with the corresponding values, known at that time. Here is an example with version string (there is a common notation, where templates have the <code>.in</code> suffix):</p> <code>version.h.in</code> <code>CMakeLists.txt</code> <pre><code>#pragma once \n\nstatic const char* cVersion = \"@MY_VERSION@\"; \n</code></pre> <pre><code>set(MY_VERSION \"1.0.5b\")\n\nconfigure_file(version.h.in ${CMAKE_CURRENT_SOURCE_DIR}/version.h)\n</code></pre> <p>This configuration will produce after \u201cgeneration\u201d step the following file:</p> <code>version.h</code> <pre><code>#pragma once \n\nstatic const char* cVersion = \"1.0.5b\";\n</code></pre>","tags":["CMake"]},{"location":"blog/19-reasons-why-cmake-is-actually-awesome/#17-cmake-allows-generation-of-doxygen-docs-without-the-hardcoded-config","title":"17. CMake allows generation of Doxygen docs without the hardcoded config","text":"<p>For some people it would be beneficial to be able to modify Doxygen configuration at build time or even completely remove it.</p> <p>CMake has a built-in function, that can add a Doxygen generation target to you build. You can also modify all settings typically found in Doxygen config by setting CMake variables with the <code>DOXYGEN_</code> prefix.</p> <p>Here is an example:</p> <pre><code>find_package(Doxygen)\nif (DOXYGEN_FOUND)\n    set(DOXYGEN_OUTPUT_DIRECTORY        ${CMAKE_BINARY_DIR}/docs) \n    set(DOXYGEN_GENERATE_LATEX          NO)\n    set(DOXYGEN_EXTRACT_PRIVATE         YES)\n    set(DOXYGEN_EXTRACT_PACKAGE         YES)\n    set(DOXYGEN_EXTRACT_STATIC          YES)\n    set(DOXYGEN_WARN_NO_PARAMDOC        YES)\n    set(DOXYGEN_HTML_OUTPUT             .)\n    set(DOXYGEN_USE_MDFILE_AS_MAINPAGE  README.md)\n    set(DOXYGEN_FILE_PATTERNS           *.c *.cc *.cxx *.cpp *.c++ *.ii *.ixx *.ipp *.i++ *.inl *.h *.hh *.hxx *.hpp *.h++ *.inc README.md)\n\n    doxygen_add_docs(doxygen . WORKING_DIRECTORY ${PROJECT_SOURCE_DIR})\nendif ()\n</code></pre>","tags":["CMake"]},{"location":"blog/19-reasons-why-cmake-is-actually-awesome/#18-cmake-automatically-detects-required-compiler-for-the-given-file","title":"18. CMake automatically detects required compiler for the given file","text":"<p>You don\u2019t have to create a separate source lists for different compilers (e.g. one for C, one for C++, one for ASM). Once you enable given language in the <code>project()</code> command CMake will automatically use the correct tool basing on the file extension.</p> <p>Here is an example of an executable built from both C and C++ files. They are all defined as one list of sources:</p> <pre><code>cmake_minimum_required(VERSION 3.15)\n\nproject(test LANGUAGES C CXX)\n\nadd_executable(testExec main.cpp moduleA.c moduleB.cpp)\n</code></pre>","tags":["CMake"]},{"location":"blog/19-reasons-why-cmake-is-actually-awesome/#19-cmake-can-create-dependency-graph","title":"19. CMake can create dependency graph","text":"<p>Software architects or technical leaders may be really interested in the dependencies between the project modules and their nature (<code>PUBLIC</code>, <code>INTERFACE</code>, <code>PRIVATE</code>). CMake can help visualize that in a form of a <code>dot</code> graph, which can be later converted into an image.</p> <p>All you need to is to pass <code>\u2014-graphviz=&lt;name&gt;.dot</code> parameter to the CMake command line and it will generate it in the build directory with the given name:</p> <pre><code>cmake &lt;PATH_TO_SOURCES&gt; --graphviz=graph/test.dot\n</code></pre> <p>Here is an example graph (<code>graph/test.dot</code>):</p> <p></p>","tags":["CMake"]},{"location":"blog/19-reasons-why-cmake-is-actually-awesome/#summary","title":"Summary","text":"<p>CMake is a powerful tool. It may have been hard to use in the past, but now with a bit of a good will it can really empower you in managing even most complicated build systems. Give it a try and I know you won\u2019t regret it.</p>","tags":["CMake"]},{"location":"blog/how-to-cross-compile-for-embedded-with-cmake-like-a-champ/","title":"How to cross-compile for embedded with CMake like a champ","text":"<p>The power of CMake lies in the fact, that it is both cross-platform in terms of build host and allows cross-compilation for different targets at the same time. In other words, we can build projects from any platform to any platform as long as we have proper tools (usually called toolchains). However many people don\u2019t know about this and try to come up with a sophisticated handmade solution or put complex logic into <code>CMakeLists.txt</code> files in order to setup the environment. Today I\u2019m going to show you how to enable cross-compilation for your project with CMake, basing on the embedded platform case.</p> <p>Note</p> <p>Here I use an embedded system example, but the described mechanisms will work for any other platform.</p> <p>Update</p> <p>Examples updated to reflect state in January 2025.</p>","tags":["CMake"]},{"location":"blog/how-to-cross-compile-for-embedded-with-cmake-like-a-champ/#how-cmake-manages-compiler","title":"How CMake manages compiler","text":"<p>Before we can switch to cross-compilation, it is crucial to understand how CMake handles compiler detection, how we can extract information about current toolchain and what is the target platform.</p> <p>If you run CMake in the console (or in IDE which shows CMake output) you can usually see, that in the beginning there are some log messages reporting what is the detected compiler. Below you can find sample output from one of my projects:</p> <pre><code>-- The ASM compiler identification is GNU\n-- Found assembler: /opt/toolchains/arm-gnu-toolchain-13.3.rel1-x86_64-arm-none-eabi/bin/arm-none-eabi-gcc\n-- The C compiler identification is GNU 13.3.1\n-- The CXX compiler identification is GNU 13.3.1\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working C compiler: /opt/toolchains/arm-gnu-toolchain-13.3.rel1-x86_64-arm-none-eabi/bin/arm-none-eabi-gcc - skipped\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: /opt/toolchains/arm-gnu-toolchain-13.3.rel1-x86_64-arm-none-eabi/bin/arm-none-eabi-g++ - skipped\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n</code></pre> <p>Here you can see, that I\u2019m using <code>arm-none-linux-gnueabihf-gcc</code> <code>13.3.1</code> C compiler and <code>arm-none-linux-gnueabihf-g++</code> <code>13.3.1</code> C++ compiler. But how does CMake know which compiler to use? After all, you can have multiple toolchains installed in your system. The answer is simple: it checks the <code>CC</code> and <code>CXX</code> environment variables. They define what is the default compiler in the system. If you manually set <code>CC</code> or <code>CXX</code> to different values, then CMake will use these new settings as default compilers. Of course, you may try to set it to something completely rubbish, but CMake will usually verify if the given compiler is usable. After successful detection, CMake stores info about the current toolchain in the following variables:</p> <ul> <li><code>CMAKE_C_COMPILER</code>,</li> <li><code>CMAKE_CXX_COMPILER</code>.</li> </ul> <p>They contain paths to the C and C++ compilers respectively. This is usually enough on desktop platforms. In the case of the embedded systems, we often need also custom linker and assembler. In the more complex projects you may need to additionally specify binaries to other parts of the toolchain (<code>size</code>, <code>ranlib</code>, <code>objcopy</code>...). All these tools should be set in the corresponding variables:</p> <ul> <li><code>CMAKE_AR</code>,</li> <li><code>CMAKE_ASM_COMPILER</code>,</li> <li><code>CMAKE_LINKER</code>,</li> <li><code>CMAKE_OBJCOPY</code>,</li> <li><code>CMAKE_RANLIB</code>.</li> </ul> <p>As for the host and target operating systems, CMake stores their names in the following variables:</p> <ul> <li>CMAKE_HOST_SYSTEM_NAME \u2013 name of the platform, on which CMake is running (host platform). On major operating systems this is set to the <code>Linux</code>, <code>Windows</code> or <code>Darwin</code> (MacOS) value.</li> <li>CMAKE_SYSTEM_NAME \u2013 name of the platform, for which we are building (target platform). By default, this value is the same as <code>CMAKE_HOST_SYSTEM_NAME</code>, which means that we are building for local platform (no cross-compilation).</li> </ul>","tags":["CMake"]},{"location":"blog/how-to-cross-compile-for-embedded-with-cmake-like-a-champ/#setting-custom-toolchain","title":"Setting custom toolchain","text":"<p>When we cross-compile to another platform, we have to specify completely different toolchain both in terms of name and location of the binaries.</p> <p>Info</p> <p>Toolchain name usually contains the so-called target triple, which is in the form of <code>&lt;arch&gt;-&lt;system&gt;-&lt;abi&gt;</code>or in a longer form <code>&lt;arch&gt;-&lt;vendor&gt;-&lt;system&gt;-&lt;abi&gt;</code>, e.g. <code>arm-linux-gnueabihf</code>. Typically, local toolchains have symbolic links that omit triple in binary name: we use <code>gcc</code> instead of the full name <code>x86_64-linux-gnu-gcc</code>.</p> <p>This can be done in two ways:</p> <ol> <li>pass values for the toolchain variables from the command line,</li> <li>specify toolchain file.</li> </ol> <p>First option is very explicit thus easy to understand, but big number of arguments makes it harder to use CMake in terminal:</p> <pre><code>cmake .. -DCMAKE_C_COMPILER=&lt;path_to_c_compiler&gt; -DCMAKE_CXX_COMPILER=&lt;path_to_cxx_compiler&gt; -DCMAKE_AR=&lt;path_to_ar&gt; -DCMAKE_LINKER=&lt;path_to_linker&gt; etc...\n</code></pre> <p>In such a case you would probably want to put it into some kind of script.</p> <p>Second option is more elegant and is the preferred way of choosing the toolchain. All you need to do is to put toolchain variables into a separate file (e.g. <code>&lt;toolchain_name&gt;.cmake</code>) and set <code>CMAKE_TOOLCHAIN_FILE</code> variable to the path of that file. This can be done both in the command line or in <code>CMakeLists.txt</code> before <code>project()</code> command:</p> <pre><code>cmake .. -DCMAKE_TOOLCHAIN_FILE=&lt;path_to_toolchain_file&gt;\n</code></pre> <p>or:</p> <pre><code>cmake_minimum_required(VERSION 3.15)\n\nset(CMAKE_TOOLCHAIN_FILE &lt;path_to_toolchain_file&gt;)\n\nproject(&lt;project_name&gt; C CXX)\n</code></pre> <p>In both cases, CMake will not try to detect default system toolchain but will blindly use whatever you provide.</p> <p>Important</p> <p>It is crucial to set the value of <code>CMAKE_TOOLCHAIN_FILE</code> before <code>project()</code> is invoked, because <code>project()</code> triggers toolchain detection and verification.</p>","tags":["CMake"]},{"location":"blog/how-to-cross-compile-for-embedded-with-cmake-like-a-champ/#structure-of-the-toolchain-file","title":"Structure of the toolchain file","text":"<p>In fact, toolchain file doesn\u2019t have any structure. You can put anything you want there. But the best practice is to define at least these settings:</p> <ul> <li>path to the toolchain binaries (C compiler, C++ compiler, linker, etc.):</li> </ul> <pre><code>set(CMAKE_AR                    &lt;path_to_ar&gt;)\nset(CMAKE_ASM_COMPILER          &lt;path_to_assembler&gt;)\nset(CMAKE_C_COMPILER            &lt;path_to_c_compiler)\nset(CMAKE_CXX_COMPILER          &lt;path_to_cpp_compiler)\nset(CMAKE_LINKER                &lt;path_to_linker&gt;)\nset(CMAKE_OBJCOPY               &lt;path_to_objcopy&gt;)\nset(CMAKE_RANLIB                &lt;path_to_ranlib&gt;)\nset(CMAKE_SIZE                  &lt;path_to_size&gt;)\nset(CMAKE_STRIP                 &lt;path_to_strip&gt;)\n</code></pre> <ul> <li>name of the target platform (and optionally target processor architecture):</li> </ul> <pre><code>set(CMAKE_SYSTEM_NAME           &lt;target_system&gt;)\nset(CMAKE_SYSTEM_PROCESSOR      &lt;target_architecture&gt;)\n</code></pre> <ul> <li>required compilation and linking flags on that particular platform:</li> </ul> <pre><code>set(CMAKE_C_FLAGS               &lt;c_flags&gt;)\nset(CMAKE_CXX_FLAGS             &lt;cpp_flags&gt;)\nset(CMAKE_C_FLAGS_DEBUG         &lt;c_flags_for_debug&gt;)\nset(CMAKE_C_FLAGS_RELEASE       &lt;c_flags_for_release&gt;)\nset(CMAKE_CXX_FLAGS_DEBUG       &lt;cpp_flags_for_debug&gt;)\nset(CMAKE_CXX_FLAGS_RELEASE     &lt;cpp_flags_for_release&gt;)\nset(CMAKE_EXE_LINKER_FLAGS      &lt;linker_flags&gt;)\n</code></pre> <ul> <li>toolchain sysroot settings:</li> </ul> <pre><code>set(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM     NEVER)\nset(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY     ONLY)\nset(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE     ONLY)\n\n# Optionally reduce compiler sanity check when cross-compiling.\nset(CMAKE_TRY_COMPILE_TARGET_TYPE         STATIC_LIBRARY)\n</code></pre> <p>Note, that in case of compilation flags I said \u201crequired\u201d. By this I mean flags that wouldn\u2019t make sense for any other platform or any other toolchain (e.g. <code>-mcpu=cortex-m4 -mfloat-abi=hard</code>). This way your toolchain file will be very generic and you will be able to reuse it in different projects and different applications built from the same codebase.</p> <p>Info</p> <p><code>CMAKE_CXX_FLAGS_DEBUG</code> and <code>CMAKE_CXX_FLAGS_RELEASE</code> have default values (e.g. <code>-O0 -g</code>). If you set new values (instead of appending them) remember, that you will lose optimization settings and debug symbols.</p> <p>Setting target platform name and architecture is less important, because it has minimal impact on CMake itself. Sometimes 3rd party modules depend on them, so it won\u2019t hurt if you set it correctly.</p> <p>One important thing to remember: if you set <code>CMAKE_SYSTEM_NAME</code> manually, CMake will automatically set <code>CMAKE_CROSSCOMPILING</code> to <code>TRUE</code> (regardless of the value you set). For example, if you compile from Windows to Windows and call <code>set(CMAKE_SYSTEM_NAME Windows)</code> before <code>project()</code>, then <code>CMAKE_CROSSCOMPILING</code> will still be <code>TRUE</code>.</p> <p>CMAKE_FIND_ROOT_PATH_MODE_PROGRAM, CMAKE_FIND_ROOT_PATH_MODE_LIBRARY and CMAKE_FIND_ROOT_PATH_MODE_INCLUDE are all related to the paths that should be searched for external packages, libraries and headers. When we are recompiling, it is crucial to tell CMake, to look for all \u201cstandard\u201d components in paths related to the current toolchain.</p>","tags":["CMake"]},{"location":"blog/how-to-cross-compile-for-embedded-with-cmake-like-a-champ/#ready-to-use-toolchain-file","title":"Ready to use toolchain file","text":"<p>If you find all the above recommendations and rules still complicated, don\u2019t worry. Here is a complete toolchain file for the embedded baremetal platform. All you need to do is to provide value for the <code>BAREMETAL_ARM_TOOLCHAIN_PATH</code> variable, pointing to the location of your toolchain (in my case it is <code>/opt/toolchains/arm-gnu-toolchain-13.3.rel1-x86_64-arm-none-eabi/bin/</code>).</p> <pre><code>set(CMAKE_SYSTEM_NAME               Generic)\nset(CMAKE_SYSTEM_PROCESSOR          arm)\n\n# Without that flag CMake is not able to pass test compilation check\nset(CMAKE_TRY_COMPILE_TARGET_TYPE   STATIC_LIBRARY)\n\nset(CMAKE_AR                        ${BAREMETAL_ARM_TOOLCHAIN_PATH}/bin/arm-none-eabi-ar)\nset(CMAKE_ASM_COMPILER              ${BAREMETAL_ARM_TOOLCHAIN_PATH}/bin/arm-none-eabi-gcc)\nset(CMAKE_C_COMPILER                ${BAREMETAL_ARM_TOOLCHAIN_PATH}/bin/arm-none-eabi-gcc)\nset(CMAKE_CXX_COMPILER              ${BAREMETAL_ARM_TOOLCHAIN_PATH}/bin/arm-none-eabi-g++)\nset(CMAKE_LINKER                    ${BAREMETAL_ARM_TOOLCHAIN_PATH}/bin/arm-none-eabi-ld)\nset(CMAKE_OBJCOPY                   ${BAREMETAL_ARM_TOOLCHAIN_PATH}/bin/arm-none-eabi-objcopy CACHE INTERNAL \"\")\nset(CMAKE_RANLIB                    ${BAREMETAL_ARM_TOOLCHAIN_PATH}/bin/arm-none-eabi-ranlib CACHE INTERNAL \"\")\nset(CMAKE_SIZE                      ${BAREMETAL_ARM_TOOLCHAIN_PATH}/bin/arm-none-eabi-size CACHE INTERNAL \"\")\nset(CMAKE_STRIP                     ${BAREMETAL_ARM_TOOLCHAIN_PATH}/bin/arm-none-eabi-strip CACHE INTERNAL \"\")\nset(CMAKE_GCOV                      ${BAREMETAL_ARM_TOOLCHAIN_PATH}/bin/arm-none-eabi-gcov CACHE INTERNAL \"\")\n\nset(COMMON_FLAGS                    \"-Wno-psabi --specs=nosys.specs -fdata-sections -ffunction-sections -Wl,--gc-sections\")\nset(CMAKE_C_FLAGS                   \"${COMMON_FLAGS} ${APP_C_FLAGS} \" CACHE INTERNAL \"\")\nset(CMAKE_CXX_FLAGS                 \"${COMMON_FLAGS} -fno-exceptions ${APP_CXX_FLAGS}\" CACHE INTERNAL \"\")\n\nset(CMAKE_C_FLAGS_DEBUG             \"-Os -g\" CACHE INTERNAL \"\")\nset(CMAKE_C_FLAGS_RELEASE           \"-Os -DNDEBUG\" CACHE INTERNAL \"\")\nset(CMAKE_CXX_FLAGS_DEBUG           \"${CMAKE_C_FLAGS_DEBUG}\" CACHE INTERNAL \"\")\nset(CMAKE_CXX_FLAGS_RELEASE         \"${CMAKE_C_FLAGS_RELEASE}\" CACHE INTERNAL \"\")\n\nset(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM NEVER)\nset(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY)\nset(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE ONLY)\n</code></pre> <p>This toolchain file is part of a more generic project called <code>platform</code>, where you can find many operating system/compiler settings for various platforms. As for the date of publishing this article, it supports the following configurations:</p> <ul> <li>(native) Linux (<code>gcc-11</code>, <code>gcc-13</code>, <code>clang-14</code>, <code>clang-18</code>),</li> <li>(cross) Linux ARM (<code>aarch64-none-linux-gnu-gcc-11</code>, <code>aarch64-none-linux-gnu-gcc-13</code>,   <code>aarch64-none-linux-gnu-clang-14</code>, <code>aarch64-none-linux-gnu-clang-18</code>),</li> <li>(cross) baremetal ARM (<code>arm-none-eabi-gcc</code>, <code>arm-none-eabi-clang</code>),</li> <li>(cross) FreeRTOS ARM (<code>arm-none-eabi-gcc</code>, <code>arm-none-eabi-clang</code>).</li> </ul>","tags":["CMake"]},{"location":"blog/how-to-cross-compile-for-embedded-with-cmake-like-a-champ/#download-platform","title":"Download platform","text":"<p><code>platform</code> project provides users with bootstrap (startup) code for each supported platform and toolchain file for that platform with a wide selection of compilers. Feel free to check and use it in your project. Once you understand how it works and its motivation, you will wonder how did you live without it.</p> <ul> <li> <p> platform</p> <p>C/C++ application bootstrap layer for various hardware/OS platforms</p> </li> </ul>","tags":["CMake"]},{"location":"blog/how-to-cross-compile-for-embedded-with-cmake-like-a-champ/#summary","title":"Summary","text":"<p>I hope this article will be helpful to you. Cross-compilation in CMake is easy and in most cases depends only on a proper toolchain file. Once provided, everything else should be platform agnostic. If you have many conditional CMake code in your project, consider extending toolchain file to hide all the OS/compiler differences there. You\u2019re welcome!</p>","tags":["CMake"]},{"location":"blog/how-to-join-repositories-in-cmake/","title":"How to join repositories in CMake","text":"<p>Sometimes there is a need in a project to use directly some other repository (local or external). This means, that we want to be able to incorporate parts (or all) of sources of the imported repository into our build system. Usually, in such a case we would also like to track which version is used at a given time.</p> <p>We can solve this problem in a few ways, e.g. by using:</p> <ul> <li>git submodules,</li> <li>git subtree,</li> <li>CMake <code>FetchContent</code>,</li> <li>CMake <code>ExternalProject</code>.</li> </ul> <p>First two are handled by the version control system and the last two are handled by the build system. Each of them has its own strengths and weaknesses, depending on the current project needs.</p> <p>Today I want to briefly present how CMake allows joining repositories with its <code>FetchContent</code> and <code>ExternalProject</code> modules.</p>","tags":["CMake"]},{"location":"blog/how-to-join-repositories-in-cmake/#fetchcontent","title":"<code>FetchContent</code>","text":"<p>Info</p> <p><code>FetchContent</code> is available since CMake 3.11.</p> <p>The role of <code>FetchContent</code> is to obtain resources from a specified location and make it available to the rest of the build system. Note, that I used the word \u201cresources\u201d which can mean not only source codes, but also toolchains, artifacts from other builds, scripts, application icons etc.</p> <p>Below you can see a typical usage:</p> <pre><code>include(FetchContent)\n\n# 1.\nFetchContent_Declare(platform\n    GIT_REPOSITORY  https://github.com/kubasejdak-org/platform.git\n    GIT_TAG         main\n)\n\n# 2.\nFetchContent_MakeAvailable(platform)\n</code></pre> <p>In this example, we are downloading my <code>platform</code> repository and using the <code>main</code> branch. Here we can see, that it consists of two parts:</p> <ol> <li>define (declare) what should be downloaded and where it is,</li> <li>launch download and make it available (populate) to the rest of the build system.</li> </ol> <p>Note, that <code>platform</code> (which I will reference later as <code>&lt;resource_name&gt;</code>) is later used as prefix in the related variables or argument in related functions, so make it meaningful and unique.</p> <p>Everything happens at the \u201cconfigure\u201d (generation) stage, once CMake reaches <code>FetchContent_MakeAvailable(platform)</code> command. This is important to remember and understand, because generation step is usually done once and <code>FetchContent</code> can influence this process. <code>FetchContent_MakeAvailable</code> command is executed only once for every resource during CMake configuration. Consider those two functions as the <code>FetchContent</code> idiom.</p> <p>Info</p> <p>Using <code>FetchContent</code> requires including proper module: <code>include(FetchContent)</code>.</p> <p>By default, everything is downloaded into your build directory in a well-defined structure:</p> <pre><code>${CMAKE_BINARY_DIR}/\n  - _deps/\n    - &lt;resource_name&gt;-build\n    - &lt;resource_name&gt;-src\n    - &lt;resource_name&gt;-subbuild\n</code></pre> <p>Note</p> <p>I\u2019m not exactly sure what is the purpose of the <code>subbuild</code> directory. After examining its contents it looks like CMake is generating <code>CMakeLists.txt</code> files there to implement <code>FetchContent</code> in terms of <code>ExternalProject</code> command. Maybe it was the easiest way of adding <code>FetchContent</code>, since <code>ExternalProject</code> was already available for a long time and can be reused at generation stage with a simple trick.</p> <p>Once CMake successfully downloads our external content, it sets two variables that can be used in <code>CMakeLists.txt</code> to locate the new data:</p> <ul> <li><code>&lt;resource_name&gt;_SOURCE_DIR</code> \u2013 specifies the location of the downloaded sources,</li> <li><code>&lt;resource_name&gt;_BINARY_DIR</code> \u2013 specifies where is the default build directory for the downloaded sources.</li> </ul> <p>However using them directly is not needed thanks to <code>FetchContent_MakeAvailable(&lt;resource_name&gt;)</code> command. It both downloads and includes that sources to our build system as if they were already part of our codebase.</p> <p>From now on, downloded directory is subject to all CMake settings of the main project and from the build system point of view is treated as the local sources.</p>","tags":["CMake"]},{"location":"blog/how-to-join-repositories-in-cmake/#externalproject","title":"<code>ExternalProject</code>","text":"<p>Info</p> <p><code>ExternalProject</code> is available since CMake 3.0.</p> <p><code>ExternalProject</code> command is almost identical to <code>FetchContent</code> in terms of the purpose and available options with, except for one extremely important difference: it is launched at build stage. In my opinion it is a crucial drawback comparing to <code>FetchContent</code> because external content is made available to us after our build system is already generated and all build decisions are already made. There is no way to add sources downloaded with this method into our compilation stage (or at least not a trivial way).</p> <p>To compensate this problem, <code>ExternalProject</code> allows launching another CMake instance on the downloaded sources and pass custom command to be used to compile it. Let\u2019s see this in an example:</p> <pre><code>include(ExternalProject)\nExternalProject_Add(platform\n    GIT_REPOSITORY  https://github.com/kubasejdak-org/platform.git\n    GIT_TAG         main\n    CMAKE_ARGS      -DPLATFORM=freertos-arm\n)\n\nadd_dependencies(&lt;some_target&gt; platform)\n</code></pre> <p>Info</p> <p>Using <code>ExternalProject</code> requires including proper module: <code>include(ExternalProject)</code>.</p> <p>The snippet above will perform the following actions:</p> <ol> <li>It will download specified repository into <code>platform-src</code> path (like in <code>FetchContent</code>).</li> <li>It will automatically call CMake in <code>platform-build</code> passing <code>-DPLATFORM=freertos-arm</code>:</li> </ol> <pre><code>cd platform-build\ncmake -DPLATFORM=freertos-arm ../platform-src\n</code></pre> <p>Note</p> <p><code>FetchContent</code> offers the same possibility.</p> <p>Making a dependency to some other target with <code>add_dependencies(&lt;some_target&gt; platform)</code> command is required in order to tell CMake when all that actions should be actually done. With <code>FetchContent</code> we didn\u2019t have that problem: generation is done sequentially in order of parsing <code>CMakeLists.txt</code> files. At build stage, CMake is composing a dependency graph of all defined targets in order to determine in which order all targets should be built. That graph is influenced by two commands:</p> <ul> <li><code>target_link_libraries()</code>, where we implicitly say that in order to build one target we need to link with another one so it would be nice if it was already built,</li> <li><code>add_dependecies()</code>, where we explicitly say, that one target should be built before another one (even if they are not using each other).</li> </ul> <p>Without making an explicit dependency between <code>ExternalProject</code> target and <code>&lt;some_target&gt;</code> CMake would simply ignore that step.</p> <p>To be honest, once <code>FetchContent</code> became available in CMake I completely lost the purpose for using <code>ExternalProject</code>.</p>","tags":["CMake"]},{"location":"blog/how-to-join-repositories-in-cmake/#customization","title":"Customization","text":"<p>Both <code>FetchContent</code> and <code>ExternalProject</code> are highly customizable. Moreover, most of the options are available in both of them. Let\u2019s see some examples along with short comments on usage.</p>","tags":["CMake"]},{"location":"blog/how-to-join-repositories-in-cmake/#download-methods","title":"Download methods","text":"<p>Both commands support the following download types:</p> <ul> <li>Git,</li> <li>SVN,</li> <li>CVS,</li> <li>Mercurial,</li> <li>HTTP.</li> </ul> <p>Each method has a typical set of settings you would normally expect, like branch name, commit/revision number, URL etc. I have personally used only Git in this context.</p> <p>While doing so, for a long time I have specified <code>main</code> branch as the content source. The problem with such declaration is that every time something changes in the content\u2019s upstream, it immediately gets populated to the places that use it. In my case, I had a few \u201cutility\u201d repositories, that were reused with <code>FetchContent</code> in highly active product repositories. For sure, using branch name relieves you from remembering to update <code>FetchContent</code> declaration in every repository once a change is introduced. But on the other side, if you are doing some breaking change, then it can create a lot of chaos. Especially if you have an active team. Trust me, you don\u2019t want to hear all those complaints. So instead of branch name, specify tag or commit hash. The only drawback of this approach is that you need to remember to update that hash whenever necessary. It is also not obvious at first sight if the changed hash is newer or older in history without checking the Git log.</p> <pre><code>include(FetchContent)\nFetchContent_Declare(platform\n    GIT_REPOSITORY  https://github.com/kubasejdak-org/platform.git\n    GIT_TAG         ee3ce49227e809aa3ef0f6270b4c996b4808cddf\n)\n\nFetchContent_MakeAvailable(platform)\n</code></pre>","tags":["CMake"]},{"location":"blog/how-to-join-repositories-in-cmake/#fetching-without-network","title":"Fetching without network","text":"<p>Most fetching methods naturally assume, that you have an Internet/Intranet connection to your remote content. However there are situations, where you are totally offline (business trips, network problems, etc). This could literally block your work.</p> <p>Fortunately there are two options you can use to work without connection to the upstream:</p> <ul> <li>using local content copy on you hard drive (e.g. clone of the repo done while connection was still available),</li> <li>forcing CMake to not try updating (checking for changes) of the content already downloaded by the previous <code>FetchContent</code>/<code>ExternalProject</code> run.</li> </ul> <p>In the first case, you can simply replace repository URL with a path to the local repository copy. It works very well. The only observable difference is the fact that content will not be physically copied into you build directory \u2013 CMake will use the existing files that you are pointing to. This should hardly ever be an issue.</p> <p>In the second case, you rely on the fact that you have successfully run <code>FetchContent</code>/<code>ExternalProject</code> at least once. Every time CMake is processing it, it checks if the content is up-to-date. In the offline scenario that check will result in an error. You can explicitly ask CMake to skip that part by adding additional parameter \u2013 <code>UPDATE_DISCONNECTED</code> with value <code>ON</code>:</p> <pre><code>include(FetchContent)\nFetchContent_Declare(platform\n    GIT_REPOSITORY        https://github.com/kubasejdak-org/platform.git\n    GIT_TAG               main\n    UPDATE_DISCONNECTED   ON\n)\n\nFetchContent_MakeAvailable(platform)\n</code></pre> <p>Note, that this will not skip downloading content for the first time. It will also not update your content if you have a healthy connection.</p> <p>Hint</p> <p>Local directory turned out to be the perfect solution if you need to make changes in fetched repository and check if it breaks dependent projects. It would be really annoying to do this using Git (it could generate a lot of trash commits).</p>","tags":["CMake"]},{"location":"blog/how-to-join-repositories-in-cmake/#choosing-the-right-place-to-run-fetchcontent","title":"Choosing the right place to \"run\" <code>FetchContent</code>","text":"<p><code>FetchContent</code> runs at the generation stage, so choosing the place where it is declared determines which parts of the build system could interact with the fetched content.</p> <p>For example, <code>FetchContent</code> can be run before <code>project()</code> function (both declare and make available statements). This could allow you to setup custom toolchain (via <code>CMAKE_TOOLCHAIN_FILE</code>) which is downloaded from the company\u2019s server.</p> <p>Note</p> <p><code>FetchContent_Declare()</code> only declares what and from where should be downloaded. <code>FetchContent_MakeAvailable()</code> actually does the job.</p>","tags":["CMake"]},{"location":"blog/how-to-join-repositories-in-cmake/#specifying-subdirectory-of-referenced-content","title":"Specifying subdirectory of referenced content","text":"<p>By default <code>FetchContent</code> blindly adds root of downloaded content to the build system. If you need to bypass it and get to the specific directory, then you have to do it by hand, e.g. my HAL project requires, that clients should use only <code>lib</code> directory, while root path is used to launch tests, generate documentation etc.:</p> <pre><code>include(FetchContent)\nFetchContent_Declare(hal    \n    GIT_REPOSITORY        https://github.com/kubasejdak-org/hal.git\n    GIT_TAG               v1.5b\n    SOURCE_SUBDIR         lib\n)\n\nFetchContent_MakeAvailable(hal)\n</code></pre>","tags":["CMake"]},{"location":"blog/how-to-join-repositories-in-cmake/#organizing-fetchcontent-declarations","title":"Organizing <code>FetchContent</code> declarations","text":"<p>I also like to put <code>FetchContent</code> commands in the separate files called <code>&lt;resource_name&gt;.cmake</code> and include them in other <code>CMakeLists.txt</code> where needed:</p> <pre><code># cmake/platform.cmake\n\ninclude(FetchContent)\nFetchContent_Declare(platform\n    GIT_REPOSITORY        https://github.com/kubasejdak-org/platform.git\n    GIT_TAG               main\n)\n\nFetchContent_MakeAvailable(platform)\n</code></pre> <pre><code># cmake/osal.cmake\n\ninclude(FetchContent)\nFetchContent_Declare(osal\n    GIT_REPOSITORY        https://github.com/kubasejdak-org/osal.git\n    GIT_TAG               main\n)\n\nFetchContent_MakeAvailable(osal)\n</code></pre> <pre><code># cmake/hal.cmake\n\ninclude(FetchContent)\nFetchContent_Declare(hal\n    GIT_REPOSITORY        https://github.com/kubasejdak/hal.git\n    GIT_TAG               main\n)\n\nFetchContent_MakeAvailable(hal)\n</code></pre> <pre><code># Root CMakeLists.txt\n\n# ...\n\ninclude(cmake/platform.cmake)\ninclude(cmake/osal.cmake)\ninclude(cmake/hal.cmake)\n\n# ...\n</code></pre>","tags":["CMake"]},{"location":"blog/how-to-join-repositories-in-cmake/#summary","title":"Summary","text":"<p>There are at least a few ways how you can join repositories in your source code. CMake offers <code>FetchContent</code> at generation stage and <code>ExternalProject</code> at build stage. Each comes with a rich set of options, which you can check in the official docs. Choose the right method for your specific case. In most cases <code>FetchContent</code> would suit you better.</p> <p>If you liked this article, then please share it on the social media you use. It will greatly increase the chance that someone else will benefit from it. It will also motivate me to write more and better articles in the future.</p>","tags":["CMake"]},{"location":"blog/introduction-to-conan-package-manager/","title":"Introduction to Conan package manager","text":"<p>In the previous article of this series, I have listed several problems that we as developers have to deal with while using dependencies to external code in our projects. These vary from version management, through cross-compilation issues, up to the horror of integration with our build system. Linux users are used to the comfort of working with package managers, which download and install applications and libraries in our host system with simple to use command-line utilities. It would be very handy to have a tool, that does something similar for libraries in the C++ world. Among many previously mentioned package managers, one has stolen my heart and attention for good \u2013 Conan.</p> In this series <ol> <li>Problems with external dependencies in C++</li> <li>Introduction to Conan package manager</li> </ol> <p>Warning</p> <p>This article is about Conan v1, which is now deprecated.</p>","tags":["Conan"]},{"location":"blog/introduction-to-conan-package-manager/#high-level-overview","title":"High level overview","text":"<p>Conan is a C/C++ package manager written in Python. This doesn\u2019t just mean, that its binary is compiled from Python. Conan is distributed as a Python package. Also, packages provided by this manager are defined in recipes in a form of Python classes. This may sound like something not important, but using one of the most popular programming languages in the world as a language for defining packages provides both tremendous flexibility as well as a very low entry-level for new users or package creators. I have experienced this myself while I was creating recipes for packages that I was missing in the existing Conan repository. I could totally focus on describing how to build a given library, rather than fight with the vocabulary of application-specific config format. Nevertheless, Python is only required if you want to create new packages. If you only want to download and use any of the existing packages, it can be done in several handy and easy ways (including Python, if this is what you want).</p> <p>In general, when we want to download any Conan package, we invoke the package manager with the name of the package. Conan is first searching registered package repositories for the ones which provide given package name. We can register multiple package repositories, which may be public or private. Also, we can use one of the existing repositories or we can set up the whole infrastructure on our own private servers. Of course, that repository needs to support Conan package protocol.</p> <p>There are a few ways of installing Conan. The most recommended one is as a Python package. In order to do that, you need to run the following command (assuming you have both <code>Python</code> and <code>pip</code> installed):</p> <pre><code>pip3 install conan\n</code></pre> <p>Once done, you will be able to search for packages with the following command:</p> <pre><code>conan search &lt;package_name&gt; -r=all\n</code></pre>","tags":["Conan"]},{"location":"blog/introduction-to-conan-package-manager/#example-using-fmt-and-nlohmann_json","title":"Example: using <code>fmt</code> and <code>nlohmann_json</code>","text":"<p>In order to quickly present you the typical flow of using Conan, we will try to build a simple C++ app that uses fmt and nlohmann_json aka <code>JSON for modern C++</code>. Both libraries are header only, but this is not the point right now. Our current situation is that we don\u2019t have it downloaded anywhere and we do not want to install it as a regular package in our system. Let\u2019s say, that the reason for that is that we want to use the latest possible version of both dependencies and our distribution is missing \u201ca few\u201d releases.</p> <p>In this example, we will try to compile the following file (<code>main.cpp</code>):</p> <pre><code>#include &lt;fmt/printf.h&gt;\n#include &lt;nlohmann/json.hpp&gt;\n\nint main()\n{\n    nlohmann::json json = {\n        {\"pi\", 3.14},\n        {\"happy\", true},\n        {\"name\", \"Kuba\"},\n        {\"nothing\", nullptr},\n        {\"answer\", {\n            {\"everything\", 42}\n        }},\n        {\"list\", {1, 2, 3}},\n        {\"object\", {\n            {\"currency\", \"PLN\"},\n            {\"value\", 100.0}\n        }}\n    };\n\n    fmt::print(\"JSON: {}\\n\", json);\n    return 0;\n}\n</code></pre> <p>As we can see, we are first creating a JSON object with the help of <code>nlohmann_json</code> library and then we are printing it with the <code>fmt</code> library. We need access to 2 header files of the mentioned libraries. In order to get it, we will create a conan \u201crequirements\u201d file, which simply lists names of the required dependencies along with their versions and also specifies how would we like to integrate it with our build system.</p> <p>Here is the <code>conanfile.txt</code> file which is kept in the project root:</p> <pre><code>[requires]\nnlohmann_json/3.9.1\nfmt/7.1.3\n\n[generators]\ncmake\n</code></pre> <p>In the <code>[requires]</code> section we list our dependencies and in the <code>[generators]</code> section we specify to which build system Conan should generate integration layer.</p> <p>Since \u201cour\u201d preferred build system is CMake, then we will of course need simple configuration for that:</p> <pre><code>cmake_minimum_required(VERSION 3.15)\n\nproject(conan_example CXX)\n\nadd_compile_options(-std=c++17)\n\ninclude(${CMAKE_BINARY_DIR}/conanbuildinfo.cmake)\nconan_basic_setup()\n\nadd_executable(conan_example\n    main.cpp\n)\n\ntarget_link_libraries(conan_example\n    PRIVATE\n        ${CONAN_LIBS}\n)\n</code></pre> <p>The final project structure is as follows:</p> <pre><code>conan_example/\n \u251c\u2500\u2500 build\n \u251c\u2500\u2500 CMakeLists.txt\n \u251c\u2500\u2500 conanfile.txt\n \u2514\u2500\u2500 main.cpp\n</code></pre> <p>In order to build the project we need to perform the following steps:</p> <ol> <li>install dependencies with Conan,</li> <li>generate build system with CMake,</li> <li>compile project with make (on Linux machine).</li> </ol>","tags":["Conan"]},{"location":"blog/introduction-to-conan-package-manager/#1-installing-dependencies-with-conan","title":"1. Installing dependencies with Conan","text":"<p>Installing dependencies using <code>conanfile.txt</code> method is very simple: execute <code>conan install &lt;path&gt;</code> from build directory. <code>&lt;path&gt;</code> should point to the location of the <code>conanfile.txt</code> file (in our example it is in the project root (one level up).</p> <pre><code>kuba@chimera:~/projects/conan_example/build$ conan install ..\nConfiguration:\n[settings]\narch=x86_64\narch_build=x86_64\nbuild_type=Release\ncompiler=gcc\ncompiler.libcxx=libstdc++11\ncompiler.version=10\nos=Linux\nos_build=Linux\n[options]\n[build_requires]\n[env]\nAR=gcc-ar-10\nAS=gcc-10\nCC=gcc-10\nCXX=g++-10\nLD=ld\nOBJCOPY=objcopy\nRANLIB=gcc-ranlib-10\nSIZE=size\nSTRIP=strip\nnlohmann_json/3.9.1: Not found in local cache, looking in remotes\u2026\nnlohmann_json/3.9.1: Trying with 'conan-center'\u2026\nDownloading conanmanifest.txt completed [0.10k]                                          \nDownloading conanfile.py completed [2.43k]                                               \nDownloading conan_export.tgz completed [0.24k]                                           \nDecompressing conan_export.tgz completed [0.00k]                                         \nnlohmann_json/3.9.1: Downloaded recipe revision 0\nconanfile.txt: Installing package\nRequirements\n    fmt/7.1.3 from 'conan-center' - Cache\n    nlohmann_json/3.9.1 from 'conan-center' - Downloaded\nPackages\n    fmt/7.1.3:b173bbda18164d49a449ffadc1c9e817f49e819d - Cache\n    nlohmann_json/3.9.1:d1091b2ed420e6d287293709a907ae824d5de508 - Download\nInstalling (downloading, building) binaries\u2026\nnlohmann_json/3.9.1: Retrieving package d1091b2ed420e6d287293709a907ae824d5de508 from remote 'conan-center' \nDownloading conanmanifest.txt completed [0.17k]                                          \nDownloading conaninfo.txt completed [0.38k]                                              \nDownloading conan_package.tgz completed [143.93k]                                        \nDecompressing conan_package.tgz completed [0.00k]                                        \nnlohmann_json/3.9.1: Package installed d1091b2ed420e6d287293709a907ae824d5de508\nnlohmann_json/3.9.1: Downloaded package revision 0\nfmt/7.1.3: Already installed!\nconanfile.txt: Generator cmake created conanbuildinfo.cmake\nconanfile.txt: Generator txt created conanbuildinfo.txt\nconanfile.txt: Generated conaninfo.txt\nconanfile.txt: Generated graphinfo\n</code></pre> <p>Here we can see a few interesting things.</p> <p>At the beginning (lines 2-23) Conan is showing the configuration that will be used to build a given set of dependencies. Of course, in the case of header-only libraries, it doesn\u2019t matter, but the same scheme will be used for any other library. This configuration includes architecture and operating system for both host and target platform (should be the same if not cross-compiling). We can also see environment variables and paths/names to the toolchain binaries. This config dump is very helpful while debugging problems. In fact, this configuration is exactly what is specified by the user in the Conan profile.</p> <p>Later, Conan is installing dependencies one by one. First, it is trying to locate a given package in the local cache. In case of the <code>nlohmann_json/3.9.1</code> library, I haven\u2019t installed it before, so there was no such entry in the cache. In such a situation, Conan is checking registered remotes one by one for the one which provides the given package. Here the <code>conan-center</code> remote (the main and registered by default Conan remote) is the one that was selected. After that, the package is downloaded and stored in the cache. By package I mean the meta data (recipe), like <code>conanfile.py</code> (line 27) which is the main description of how to build a given library.</p> <p>Just before executing the build, Conan is once again explicitly showing what packages were selected (along with their unique ids) and where they were located \u2013 cache or remotes (lines 32-37). Finally, it starts building, which in case of the header only libraries results in simply downloading and putting files into the proper cache location.</p> <p>In the end, Conan is generating an integration layer for our build system, as it was specified in the <code>[generators]</code> section of the <code>conanfile.txt</code> file. In our case, it is creating files that we will have to include in our <code>CMakeLists.txt</code> in order for CMake to properly locate libraries installed into the Conan cache (lines 47-50). In particular, the most important file for us will be the <code>conanbuildinfo.cmake</code> (see snippet with <code>CMakeLists.txt</code> lines 7-8).</p>","tags":["Conan"]},{"location":"blog/introduction-to-conan-package-manager/#2-generating-build-system-with-cmake","title":"2. Generating build system with CMake","text":"<p>Now is the time to launch CMake. We do this in a typical way \u2013 nothing fancy here.</p> <pre><code>kuba@chimera:~/projects/conan_example/build$ cmake ..\n-- The CXX compiler identification is GNU 10.2.0\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Conan: Adjusting output directories\n-- Conan: Using cmake global configuration\n-- Conan: Adjusting default RPATHs Conan policies\n-- Conan: Adjusting language standard\n-- Current conanbuildinfo.cmake directory: /home/kuba/projects/conan_example/build\n-- Conan: Compiler GCC&gt;=5, checking major version 10\n-- Conan: Checking correct version: 10\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/kuba/projects/conan_example/build\n</code></pre> <p>Assuming that we correctly installed dependencies with Conan, included the <code>conanbuildinfo.cmake</code> file and called <code>conan_basic_setup()</code> (defined in <code>conanbuildinfo.cmake</code>) there should be some confirmation logs about Conan in the CMake output (lines 8-14).</p> <p>Note, that the CMake generator creates a variable <code>CONAN_LIBS</code> which groups all the CMake targets that represent installed dependencies. The only thing that we need to do is to use this helper variable in the <code>target_link_libraries()</code> command (<code>CMakeLists.txt</code> \u2013 lines 14-16).</p> <p>Note</p> <p>This example is extremely simple and definitely not sufficient for every use/project. Putting all dependencies into one variable (<code>CONAN_LIBS</code>) is something that I personally never used while working with Conan. There are other, more flexible ways of doing so, which will be discussed in the next articles.</p>","tags":["Conan"]},{"location":"blog/introduction-to-conan-package-manager/#compile-project-with-make-on-linux-machine","title":"Compile project with make (on Linux machine)","text":"<p>Finally, we can build application with the native toolset:</p> <pre><code>kuba@chimera:~/projects/conan_example/build$ make\nScanning dependencies of target conan_example\n[ 50%] Building CXX object CMakeFiles/conan_example.dir/main.cpp.o\n[100%] Linking CXX executable bin/conan_example\n[100%] Built target conan_example\n</code></pre> <p>The output of the program is like we expect:</p> <pre><code>JSON: {\"answer\":{\"everything\":42},\"happy\":true,\"list\":[1,2,3],\"name\":\"Kuba\",\"nothing\":null,\"object\":{\"currency\":\"PLN\",\"value\":100.0},\"pi\":3.14}\n</code></pre>","tags":["Conan"]},{"location":"blog/introduction-to-conan-package-manager/#custom-settings","title":"Custom settings","text":"<p>Conan\u2019s advantage over many existing package managers is that it allows us to customize many different settings that can relate to tools we want to use, target platform or even from where packages should be searched and downloaded. Everything that user can customize is cleanly organized in specific configuration files. They usually have easy to understand text format (like JSON or INI-ish). You can edit it manually (if you know the syntax) or use Conan commands to modify them for you. The latter option is quite handy if you need to adjust configs automatically in the Continous Integration system.</p> <p>There are two user-specific types of options that typically you want to adjust: profiles and remotes. Below you can find more info about each of them.</p>","tags":["Conan"]},{"location":"blog/introduction-to-conan-package-manager/#profiles","title":"Profiles","text":"<p>Conan profiles are the heart of the user configuration for Conan. They specify many different variables, that Conan will use during the building process:</p> <ul> <li>names/paths of the toolchain,</li> <li>metadata of the host and target platforms,</li> <li>environment variables,</li> <li>package-specific options (options that are provided by concrete package creator, e.g. \u201cinclude tests\u201d or \u201cenable   feature A\u201d).</li> </ul> <p>Many of the above options can be either set globally or for the given package (e.g. use higher optimization for <code>spdlog</code> package).</p> <p>This feature itself is very powerful, because you can create as many profiles as you want and give them some meaningful name which will reflect the contents. For example, you can have a profile which allows compilation with GCC, clang, with full debug mode, with high optimization, profile which is used to cross-compile for Raspberry Pi etc.</p> <p>Even more possibilities are achievable by composing profiles together. So if you have a profile that only specifies compiler (e.g. GCC or clang), a separate one which selects build mode (debug or release) and finally separate specific for the operating system (e.g. Windows and Linux) then you can combine them together by simply listing their names in the command line, for example:</p> <pre><code>conan install .. -pr gcc-10 -pr debug -pr linux\n</code></pre> <p>I personally define one profile per concrete toolchain (which includes target operating system). Here is my profile for the GCC 10 compiler (named <code>gcc-10</code>):</p> <pre><code>[settings]\n os=Linux\n os_build=Linux\n arch=x86_64\n arch_build=x86_64\n compiler=gcc\n compiler.version=10\n compiler.libcxx=libstdc++11\n build_type=Release\n [options]\n [build_requires]\n [env]\n AR=gcc-ar-10\n AS=gcc-10\n CC=gcc-10\n CXX=g++-10\n OBJCOPY=objcopy\n RANLIB=gcc-ranlib-10\n SIZE=size\n STRIP=strip\n ```\n\nAs you can see above, in the `[settings]` section I am defining metadata for the platform (operating system,\narchitecture, build type and compiler type). In `[options]` and `[build_requires]` I do not specify anything\npackage-related, because this profile is generic \u2013 it only selects proper toolchain). In the `[env]` section I specify\nwell-known variables that are used by all popular build systems. This way, it doesn\u2019t matter if the given package is\ncompiled using autotools or CMake \u2013 it simply uses given variables.\n\nBelow you can find a list of my personal profiles:\n\n```bash\nkuba@chimera:~$ conan profile list\n arm-linux-gnueabihf-clang-11\n arm-linux-gnueabihf-gcc-10\n arm-none-eabi-gcc-10\n clang-11\n default\n gcc-10\n</code></pre> <p>Each profile is kept as a separate file in <code>~/.conan/profiles/&lt;profile_name&gt;</code>. During the first usage Conan will detect your current operating system setup and generate a <code>default</code> profile for you. <code>default</code> profile is used by default if you do not specify any other in the command line (or in other way).</p>","tags":["Conan"]},{"location":"blog/introduction-to-conan-package-manager/#remotes","title":"Remotes","text":"<p>Remotes are another user-specific settings that you may want to modify. They are simply the names and URLs of the Conan servers that are used to search for the packages. As it was shown with the <code>fmt</code> and <code>nlohmann_json</code> example, every time a package is installed, Conan is searching registered remotes one by one and checking if any of them is providing given package in a specified version.</p> <p>By default, the only registered remote is the conan-center, which is the main official Conan repository. Every day Conan maintainers are adding there new packages and updating the existing ones.</p> <p>However, you can register remotes owned by other people, organizations or even your company. It doesn\u2019t matter if given server is public or private. As long as you have access rights then you are good to go. So in other words, Conan is a perfect choice for corporations, that need to keep their stuff private but still provide a central place for their packages. I have registered my own repository to keep packages, that maybe are not generic enough to upload to the official repository, but are very useful for me. I also use it as a testing ground for packages that may eventually land in the <code>conan-center</code> once I test them enough.</p> <p>Here is the list of remotes registered on my machine:</p> <pre><code>kuba@chimera:~$ conan remote list\n conan-center: https://conan.bintray.com [Verify SSL: True]\n bincrafters: https://api.bintray.com/conan/bincrafters/public-conan [Verify SSL: True]\n kubasejdak: https://api.bintray.com/conan/kubasejdak/public-conan [Verify SSL: True]\n</code></pre> <p>Conan servers are holding two types of data: package recipes (metadata) and prebuilt binaries. In a typical workflow, Conan is first checking the local cache for the package and then the registered remotes. But not always downloading package from a registry must result in compilation on the client machine. Sometimes package maintainers create prebuilt binaries for the most popular configurations and upload them to the remotes. Conan is checking if the currently selected profile (mentioned above) has a corresponding prebuilt package on the server. If yes, then it downloads the binary and saves you time. However, you can force Conan to always compile dependencies on your machine.</p>","tags":["Conan"]},{"location":"blog/introduction-to-conan-package-manager/#installing-packages","title":"Installing packages","text":"<p>Conan packages can be installed in at least three different ways:</p> <ol> <li>manually,</li> <li>automatically with <code>conanfile.txt</code>,</li> <li>automatically with CMake wrapper.</li> </ol> <p>Each of them has its purpose, but usually you will use either <code>conanfile.txt</code> or CMake wrapper. This is because you typically what to automate dependency installation while you are building your project. Let\u2019s see a brief overview of what you can do in each of these ways.</p>","tags":["Conan"]},{"location":"blog/introduction-to-conan-package-manager/#manual-installation-from-command-line","title":"Manual installation from command line","text":"<p>Manual package installation means that you explicitly invoke Conan to install explicitly defined set of libraries in the command line. A simple example would be as follows:</p> <pre><code>conan install opencv/4.5.1@\n</code></pre> <p>Here we want to install <code>opencv</code> package in version <code>4.5.1</code>. We didn\u2019t specify any profile so Conan will use the default one. We also didn\u2019t specify any generator, so no build system integration will be generated. This looks simple. It is an easy way to experiment, check your own packages or test out different libraries.</p> <p>That command can be extended to include package-specific settings, required profile or generator that we want to use:</p> <pre><code>conan install opencv/4.5.1@ -o opencv:with_png=False -o opencv:with_tiff=False -o opencv:with_gtk=False -pr gcc-10 -g cmake\n</code></pre> <p>That line can quickly expand if we compose multiple profiles, libraries and options. Not to mention, that you can override specific settings from selected profiles. You will quickly realize, that this is not a way for production code. However, if you are experimenting or looking for a proper library to include in your project then this is the quickest way of getting started.</p>","tags":["Conan"]},{"location":"blog/introduction-to-conan-package-manager/#automatic-installation-with-conanfiletxt","title":"Automatic installation with <code>conanfile.txt</code>","text":"<p>Conan is supporting the idea taken from Python package manager (<code>pip</code>), which allows putting all requirements and options into a single file \u2013 the conanfile.txt. The above example with <code>opencv</code> could look like this in <code>conanfile.txt</code>:</p> <pre><code>[requires]\nopencv/4.5.1\n\n[generators]\ncmake\n\n[options]\nopencv:with_png=False\nopencv:with_tiff=False\nopencv:with_gtk=False\n</code></pre> <p>Here we have a bit more readable format than in the command line, because of the separate sections for each setting. However, the biggest advantage of this, is that you can put all these configurations in a file and have it under the control of the VCS. This way you will always have the correct set of dependencies for the currently checked-out code.</p> <p>You can also have a set of files that contain different libraries. This way, by feeding Conan with a different path to the <code>conanfile.txt</code> you can install another set of packages. It may be helpful if you build many applications from the same codebase. Usually, each application has its own set of dependencies.</p>","tags":["Conan"]},{"location":"blog/introduction-to-conan-package-manager/#automatic-installation-with-cmake-wrapper","title":"Automatic installation with CMake wrapper","text":"<p>The canonical (recommended) way of using Conan is via the <code>conanfile.txt</code> configuration. This is fine as long as your dependencies are not dynamically changing. It can happen if you have, for example, a library that can be built in different configurations. And each config may require a separate set of dependencies. Let\u2019s look at this simple example:</p> <pre><code>mylib/\n \u251c\u2500\u2500 CMakeLists.txt\n \u251c\u2500\u2500 module_a\n \u2502   \u2514\u2500\u2500 NetworkBackend.cpp # requires \"libcurl\" library\n \u2514\u2500\u2500 module_b\n     \u2514\u2500\u2500 DatabaseBackend.cpp # requires \"sqlite3\" library\n</code></pre> <p>Here we have a directory structure for the <code>mylib</code> library which consists of two submodules: <code>module_a</code> and <code>module_b</code>. The first one is related to some network stuff so it needs <code>libcurl</code> in its implementation. The second one operates on a local database and requires <code>sqlite3</code> library. User application, which links with <code>mylib</code>, can either compile it with <code>module_a</code>, or <code>module_b</code> or both. How can we instruct Conan to actually co-operate with CMake and install only was is needed in the given setup?</p> <p>If this example is still not convincing, then imagine that there are hundreds of such modules that internally can also be built from different parts depending on other settings. Also, we may have some parts of code in our library which are platform-specific and require libraries only for that platform. We do not want to be caught in a situation, where Conan is trying to install Linux libraries on Windows.</p> <p>At the top of it, the application (which is only using the <code>mylib</code> library) can have its own dependencies. So we have a lot of potential sources of libraries to be installed. And the final set of packages depends on the build system. If you remember the first example with <code>fmt</code> and <code>nlohmann_json</code>, you know that the order of steps is as follows:</p> <ol> <li>install dependencies,</li> <li>launch CMake,</li> <li>compile.</li> </ol> <p>In our situation, CMake has to be launched first. But even that is not enough, because CMake may want to verify that libraries that we want to use are actually available. So Conan needs to install the libraries before CMake is finished. Impossible to solve? No, there is a handy solution \u2013 cmake-conan wrapper.</p> <p>This script is a CMake file containing helper functions to actually execute Conan from CMake. This way, you can collect dependencies in a list as CMake parses your project. Then, in the end you can call Conan from the build system and install all libraries at once. Alternatively, you can install ad-hoc dependencies in the <code>CMakeLists.txt</code> file, which actually needs it. The second approach is much better because you can both guarantee that you install only what is actually used and you can use well-known CMake mechanisms (like <code>find_library</code>) that require the library to be installed apriori.</p> <p>Here is a very simple example of how <code>cmake-conan</code> wrapper can be used:</p> <pre><code>include(conan.cmake) # This it the actual cmake-conan wrapper file.\n\nconan_cmake_run(\n    REQUIRES opencv/5.4.1\n    OPTIONS opencv:with_png=False opencv:with_tiff=False opencv:with_gtk=False\n)\n</code></pre> <p><code>conan_cmake_run()</code> is actually calling Conan command and waiting for the result. Since you can use CMake variables in that call, this mechanism povides tremendous flexibility in defining which packages should be installed.</p> <p>Note</p> <p>I am personally using yet another approach, based on the CMake wrapper. Details will be explained in the next     article in the series.</p>","tags":["Conan"]},{"location":"blog/introduction-to-conan-package-manager/#conan-cache","title":"Conan cache","text":"<p>As I mentioned before, Conan creates a local cache for all downloaded and built packages for future reusing. Typically, it is located in <code>$HOME/.conan/data</code> directory. Below you can see sample cache state from my computer:</p> <pre><code>kuba@chimera:~$ tree .conan/data/ -L 5 -d\n .conan/data/\n \u251c\u2500\u2500 catch2\n \u2502   \u251c\u2500\u2500 2.13.2\n \u2502   \u2502   \u2514\u2500\u2500 _\n \u2502   \u2502       \u2514\u2500\u2500 _\n \u2502   \u2502           \u251c\u2500\u2500 export\n \u2502   \u2502           \u2514\u2500\u2500 package\n \u2502   \u2514\u2500\u2500 2.13.3\n \u2502       \u2514\u2500\u2500 _\n \u2502           \u2514\u2500\u2500 _\n \u2502               \u251c\u2500\u2500 dl\n \u2502               \u251c\u2500\u2500 export\n \u2502               \u251c\u2500\u2500 locks\n \u2502               \u2514\u2500\u2500 package\n \u251c\u2500\u2500 fmt\n \u2502   \u2514\u2500\u2500 7.1.3\n \u2502       \u2514\u2500\u2500 _\n \u2502           \u2514\u2500\u2500 _\n \u2502               \u251c\u2500\u2500 dl\n \u2502               \u251c\u2500\u2500 export\n \u2502               \u251c\u2500\u2500 locks\n \u2502               \u2514\u2500\u2500 package\n \u251c\u2500\u2500 grpc\n \u2502   \u2514\u2500\u2500 1.24.3\n \u2502       \u2514\u2500\u2500 kubasejdak\n \u2502           \u2514\u2500\u2500 stable\n \u2502               \u251c\u2500\u2500 build\n \u2502               \u251c\u2500\u2500 export\n \u2502               \u251c\u2500\u2500 export_source\n \u2502               \u251c\u2500\u2500 locks\n \u2502               \u251c\u2500\u2500 package\n \u2502               \u2514\u2500\u2500 source\n \u251c\u2500\u2500 libgpiod\n \u2502   \u2514\u2500\u2500 1.4.3\n \u2502       \u2514\u2500\u2500 kubasejdak\n \u2502           \u2514\u2500\u2500 stable\n \u2502               \u251c\u2500\u2500 build\n \u2502               \u251c\u2500\u2500 export\n \u2502               \u251c\u2500\u2500 export_source\n \u2502               \u251c\u2500\u2500 locks\n \u2502               \u251c\u2500\u2500 package\n \u2502               \u2514\u2500\u2500 source\n \u2514\u2500\u2500 nlohmann_json\n     \u2514\u2500\u2500 3.9.1\n         \u2514\u2500\u2500 _\n             \u2514\u2500\u2500 _\n                 \u251c\u2500\u2500 dl\n                 \u251c\u2500\u2500 export\n                 \u251c\u2500\u2500 locks\n                 \u2514\u2500\u2500 package\n</code></pre> <p>Each installed package in each version has its own subdirectory there. Conan is first checking if the given dependency is already in the cache, before downloading the recipe. Knowledge about the cache and its structure is very helpful when you are creating your own packages.</p>","tags":["Conan"]},{"location":"blog/introduction-to-conan-package-manager/#summary","title":"Summary","text":"<p>Conan is a powerful package manager for C and C++. It\u2019s true, that at first many things may seem complicated or overwhelming. But this is the cost of introducing something new. I can only say, that in time using this tool will become very easy and you will be quickly tempted to add new packages yourself, once you find any which is not already provided by the official repository.</p> <p>In the next article I will focus more on cross-compilation and I will show how I use Conan in my CMake projects. For that, I am using a little modified version of the <code>cmake-conan</code> wrapper. Stay tuned!</p>","tags":["Conan"]},{"location":"blog/let-the-games-begin/","title":"Let the games begin","text":"<p>Hello hello! My name is Kuba Sejdak. I\u2019m a C++ software engineer for over 7 years. Today I realized, that I have a need to share my C++ journey with the world. So I decided to start a blog.</p> <p>In fact, this is not my first time. Over a year ago I tried to start another blog focused around usage of C++ in the embedded systems. I didn\u2019t have much knowledge to share at that time. My idea was to start an ambitious embedded project and document everything on the blog. This way I would have been forced to go deeper into the subject and fully understand things. The reality was harder than I thought. This topic is so advanced and so niche, that I was doing only the R&amp;D part. Website was left alone until the next payment for the hosting. Finally I decided to stop that madness and abandon the blogging stuff.</p> <p>Hello hello! My name is Kuba Sejdak. I\u2019m a C++ software engineer for over 7 years. Today I realized, that I have a need to share my C++ journey with the world. So I decided to start a blog.</p> <p>In fact, this is not my first time. Over a year ago I tried to start another blog focused around usage of C++ in the embedded systems. I didn\u2019t have much knowledge to share at that time. My idea was to start an ambitious embedded project and document everything on the blog. This way I would have been forced to go deeper into the subject and fully understand things. The reality was harder than I thought. This topic is so advanced and so niche, that I was doing only the R&amp;D part. Website was left alone until the next payment for the hosting. Finally I decided to stop that madness and abandon the blogging stuff.</p> <p>Now I don\u2019t want to focus on the particular thing. I will write only about things that I know, when time is right. No pressure. Let\u2019s see how will that go. I\u2019m sure it will be much better. Wish me luck and I hope to see you soon! Let the games begin!</p>"},{"location":"blog/modern-cmake-is-like-inheritance/","title":"Modern CMake is like inheritance","text":"<p>CMake has been created in 2000 by Bill Hoffman from Kitware. During the last 20 years, as of the time of this publication, it\u2019s been constantly evolving by adding new features and expanding its support for third-party libraries. But the most significant additions were released in version 3.0 and are commonly called \u201cModern CMake\u201d. Despite being available for more than 5 years, many programmers are still not using them!</p> <p>Today I will show you one of the greatest \u201cModern CMake\u201d feature, that behaves almost like C++ inheritance. But before we get there, let me briefly explain a few fundaments.</p>","tags":["CMake"]},{"location":"blog/modern-cmake-is-like-inheritance/#modern-cmake-targets-properties","title":"Modern CMake = targets + properties","text":"<p>Target is a fundamental concept in CMake. It generally represents one \u201cjob\u201d of the building process. Most commonly used targets are executables and libraries. Let\u2019s see the syntax for creating and using them:</p> <pre><code>add_executable(myExecutable\n    main.cpp\n)\n\nadd_library(libA\n    sourceA.cpp\n)\n\nadd_library(libB\n    sourceB.cpp\n    sourceB_impl.cpp\n)\n\nadd_library(libC\n    sourceC.cpp\n)\n\ntarget_link_libraries(myExecutable libA)\ntarget_link_libraries(libA libB)\ntarget_link_libraries(libB libC)\n</code></pre> <p>This is a classic way of creating targets. Here we have an executable called <code>myExecutable</code> and three libraries <code>libA</code>, <code>libB</code> and <code>libC</code>. For now we know, that in order to build <code>myExecutable</code> we have to link with <code>libA</code>, for building <code>libA</code> we have to link with <code>libB</code> and for building <code>libB</code> we have to link with <code>libC</code>. The last one has no dependencies.</p> <p>Let\u2019s keep in mind, that those are not the only possible targets that can be defined in CMake. We can create also custom targets, whose only role is to invoke some command:</p> <pre><code>add_custom_target(firmware.bin\n    COMMAND             ${CMAKE_OBJCOPY} -O binary firmware firmware.bin\n    DEPENDS             firmware\n    WORKING_DIRECTORY   \"${CMAKE_BINARY_DIR}/bin\"\n)\n</code></pre> <p>In the snippet above we are defining a custom target named <code>firmware.bin</code>, whose only objective is to create a BIN file from the original executable using GNU objcopy (if you are not familiar with objcopy, then think of this as some conversion of the binary file). Here we explicitly say, that <code>firmware.bin</code> depends on <code>firmware</code>. This is natural, because in order to convert firmware file it must be already built!</p> <p>Note</p> <p>We could also skip the line with <code>DEPENDS</code> and add <code>add_dependencies(firmware.bin firmware)</code> instead as a separate statement. But in my opinion, it is less elegant.</p> <p>Each target can have its own set of properties, that will be used in proper contexts. Here is a list of the most popular ones:</p> <ul> <li>compilation flags,</li> <li>linking flags,</li> <li>preprocessor flags,</li> <li>C/C++ standard,</li> <li>include directories.</li> </ul> <p>All of the above properties are stored in the special CMake variables and are automatically used by the build system, when the given target appears in the certain context. For example, include directories property is automatically added to the compilation flags when the target is being compiled. Linking flags are automatically passed to the linker, when this target is being linked. You may say, that all these properties are either <code>CFLAGS</code> or <code>LDFLAGS</code> so there is no need to extract them into individual entities. But we will see in a moment, that this separation can be quite handy in Modern CMake projects.</p>","tags":["CMake"]},{"location":"blog/modern-cmake-is-like-inheritance/#setting-properties-include-directories-preprocessor-compilation-and-linking-flags","title":"Setting properties: include directories, preprocessor, compilation and linking flags","text":"<p>Target properties can be set in at least few ways. Before CMake 3.x we could either set raw CMake flags (e.g. <code>CMAKE_C_FLAGS</code>, <code>CMAKE_LINKER_FLAGS</code>) or use directory-oriented commands, which set given property for all targets in the current directory and its subdirectories. Modern CMake introduced a new set of target-oriented commands, which allow us to set properties for targets individually.</p> <p>Below you can find a comparison of the old directory-oriented commands and their new recommended alternatives in Modern CMake:</p> DIRECTORY-ORIENTED OLD COMMANDS TARGET-ORIENTED NEW COMMANDS <code>include_directories(&lt;include_path&gt;)</code> <code>target_include_directories(&lt;target&gt; [VISIBILITY] &lt;include_path&gt;)</code> <code>add_definitions(&lt;preprocessor_flags&gt;)</code> <code>target_compile_definitions(&lt;target&gt; [VISIBILITY] &lt;preprocessor_flags&gt;)</code> <code>set(CMAKE_CXX_FLAGS &lt;compilation_flags&gt;)</code> <code>target_compile_options(&lt;target&gt; [VISIBILITY] &lt;compilation_flags&gt;)</code> <code>set(CMAKE_LINKER_FLAGS &lt;linker_flags&gt;)</code> <code>target_link_options(&lt;target&gt; [VISIBILITY] &lt;linker_flags&gt;)</code> <p>Note</p> <p>I personally prefer to indent each entry after <code>[VISIBILITY]</code> and put each one on a separate line like this: <pre><code>target_include_directories(&lt;target&gt;\n    [VISIBILITY]\n        &lt;include_path1&gt;\n        &lt;include_path2&gt;\n)\n</code></pre></p> <p>Modern CMake introduced also new keywords, that specify visibility of the given target property: <code>PRIVATE</code>, <code>PUBLIC</code>, <code>INTERFACE</code>. Their meanings are as follows:</p> <ul> <li><code>PRIVATE</code> property is only for the internal usage by the property owner,</li> <li><code>PUBLIC</code> property is for both internal usage by the property owner and for other targets that use it (link with it),</li> <li><code>INTERFACE</code> property is only for usage by other libraries.</li> </ul> <p>This is very similar to the access specifiers in a C++ class! Each of the new commands allow visibility specification. If none is provided, then <code>PUBLIC</code> is assumed. Note, that each command can have multiple properties set at different visibility level:</p> <pre><code>target_include_directories(&lt;target&gt;\n    INTERFACE\n        &lt;include_path_1&gt;\n        &lt;include_path_2&gt;\n        &lt;include_path_3&gt;\n    PUBLIC\n        &lt;include_path_4&gt;\n        &lt;include_path_5&gt;\n        &lt;include_path_6&gt;\n    PRIVATE\n        &lt;include_path_7&gt;\n        &lt;include_path_8&gt;\n        &lt;include_path_9&gt;\n)\n</code></pre> <p>Note</p> <p>Targets that don\u2019t produce any binaries (e.g. header-only libraries) can have only <code>INTERFACE</code> properties and can only use <code>INTERFACE</code> linking. This is quite understandable, because there is no \u201cinternal\u201d part in such targets, so <code>PRIVATE</code> keyword doesn\u2019t mean anything.</p>","tags":["CMake"]},{"location":"blog/modern-cmake-is-like-inheritance/#using-linking-with-libraries-behaves-like-inheritance","title":"Using (linking with) libraries behaves like inheritance","text":"<p>In order to link libraries together we use the expression:</p> <pre><code>target_link_libraries(&lt;TARGET_A&gt; &lt;TARGETS...&gt;)\n</code></pre> <p>Modern CMake extended this command with the visibility specifier like this:</p> <pre><code>target_link_libraries(&lt;TARGET_A&gt; [VISIBILITY] &lt;TARGETS...&gt;)\n</code></pre> <p>Again, visibility can be one of <code>PRIVATE</code>, <code>PUBLIC</code> and <code>INTERFACE</code>. If none is provided then <code>PUBLIC</code> is used by default. But what does it mean in terms of linking?</p> <p>CMake 3.x introduced a very important \u201cside effect\u201d of linking with targets: linked target is passing all its <code>PUBLIC</code> and <code>INTERFACE</code> properties to the library that it links with. So for example, if <code>libA</code> is linking with <code>libB</code> then <code>libA</code> gets all <code>PUBLIC</code> and <code>INTERFACE</code> properties of <code>libB</code>. <code>PRIVATE</code> properties are still not accessible. Another question is: what is the visibility of the newly obtained set of properties (by <code>libA</code>)? Answer is simple: it\u2019s the same as the specifier used in <code>target_link_libraries()</code> for that target. So if <code>libA</code> links as <code>PRIVATE</code> with <code>libB</code>, then all <code>PUBLIC</code> and <code>INTERFACE</code> properties of <code>libB</code> become <code>PRIVATE</code> properties of <code>libA</code>. Similarly, if it links as <code>PUBLIC</code>, then all <code>PUBLIC</code> and <code>INTERFACE</code> properties of <code>libB</code> become <code>PUBLIC</code> in <code>libA</code>. The same goes for <code>INTERFACE</code> linking.</p> <p>Can you see now, that this looks almost identical as inheritance in C++? Private inheritance makes all public and protected members private in the derived class and public inheritance keeps the visibility unchanged.</p> <p>In order to understand it better, let\u2019s see some use cases.</p>","tags":["CMake"]},{"location":"blog/modern-cmake-is-like-inheritance/#example-1-avoiding-header-dependencies","title":"Example 1: avoiding header dependencies","text":"<p>Let\u2019s assume the following directory structure:</p> <pre><code>libA/\n    - include/\n        - libA/\n            - sourceA.h\n    - privateHeaderA1.h\n    - privateHeaderA2.h\n    - sourceA.cpp\nlibB/\n    - include/\n        - libB/\n            - sourceB.h\n    - submodule/\n        - submodule.h\n        - submodule.cpp\n    - privateHeaderB1.h\n    - privateHeaderB2.h\n    - sourceB.cpp\n    - sourceB_impl.h\n    - sourceB_impl.cpp\nlibC/\n    - include/\n        - libC/\n            - sourceC.h\n    - privateHeaderC1.h\n    - privateHeaderC2.h\n    - sourceC.cpp\nmain.cpp\n</code></pre> <p>and the following include dependencies in code:</p> <pre><code>// sourceB.cpp\n\n#include \"libC/sourceC.h\"\n#include \"submodule.h\"\n\n// ...\n</code></pre> <pre><code>// sourceA.h\n\n#include \"libB/sourceB.h\"\n\n// ...\n</code></pre> <pre><code>// main.cpp\n\n#include \"libA/sourceA.h\"\n#include \"libC/sourceC.h\"\n\n// ...\n</code></pre> <p>Also let\u2019s define a rule, that we don't want any library to be able to use \"private\" headers of the other libraries: e.g. the code below shouldn\u2019t compile:</p> <pre><code>// main.cpp\n\n#include \"privateHeaderC2.h\" // should fail as \"no such file or directory\"\n</code></pre> <p>This restriction is a good architectural practice, that can keep the code clean from unwanted dependencies. How to make that compile without a messy config?</p> <p>First we have to check each target and determine the include paths that it is \u201ccreating\u201d. By this I mean which include paths belong to this particular target. Then for each path in a given target we have to decide, if it should be accessible by others (<code>PUBLIC</code>) or not (<code>PRIVATE</code>). Finally we will use new target-oriented commands to set the include properties for each library.</p> <pre><code>add_executable(myExecutable\n    main.cpp\n)\n</code></pre> <pre><code>add_library(libA\n    sourceA.cpp\n)\ntarget_include_directories(libA\n    PUBLIC\n        include\n)\n</code></pre> <pre><code>add_library(libB\n    sourceB.cpp\n    submodule/submodule.cpp\n)\n\ntarget_include_directories(libB\n    PUBLIC\n        include\n    PRIVATE\n        submodule\n)\n</code></pre> <pre><code>add_library(libC\n    sourceC.cpp\n)\n\ntarget_include_directories(libC\n    PUBLIC\n        include\n)\n</code></pre> <p>All these targets have one thing in common: the only <code>PUBLIC</code> include path is the <code>include</code> directory. This means, that if other libraries call only <code>target_link_libraries()</code> to both get include paths and link with library, then no private header will ever leak unintentionally outside the containing library.</p> <p>Now its time to properly link the libraries:</p> <pre><code>target_link_libraries(myExecutable\n    PRIVATE\n        libA\n        libC\n)\n\ntarget_link_libraries(libA\n    PUBLIC\n        libB\n)\n\ntarget_link_libraries(libB\n    PRIVATE\n        libC\n)\n</code></pre> <p>Observe the following things:</p> <ol> <li>Executable doesn\u2019t need to specify linking type (because nothing can link with exec), but we define it for consistency.</li> <li><code>libA</code> links publicly with <code>libB</code>, because it is using header from <code>libB</code> in its own public header. So it has to provide this path to its clients.</li> <li><code>libB</code> links privately with <code>libC</code>, because it is using header from <code>libC</code> only in its internal implementation and its clients shouldn\u2019t even be aware of this.</li> <li><code>target_link_libraries()</code> means in Modern CMake two things: use library (get its properties) at compilation stage and link with it at linking stage. Hence maybe a bit better name for it would be <code>target_use_libraries()</code> but it would break the backward compatibility.</li> </ol>","tags":["CMake"]},{"location":"blog/modern-cmake-is-like-inheritance/#example-2-defining-header-only-libraries","title":"Example 2: defining header-only libraries","text":"<p>Sometimes we have to deal with libraries, that don\u2019t produce any binaries. For example, they are just a set of headers that your application needs to include. In such a case they are called a header-only libraries.</p> <p>An excellent example would be the Catch2 library, which implements the popular C++ testing framework. It consists of only one file <code>catch.hpp</code> which is stored in <code>catch2</code> directory. First, it would be convenient for us, to still have a CMake target that provides path to that file once someone links with it. Secondly, Catch2 allows some behavior customization via the define directives. For example, we can disable usage of POSIX signals and exceptions in favor of a call to <code>std::terminate()</code>. This is particularly crucial on embedded systems, where we can\u2019t use any of them. So our target should also be able to detect the environment and provide the proper defines accordingly.</p> <p>In Modern CMake it could be expressed like this:</p> <pre><code>add_library(catch2 INTERFACE)\n\ntarget_include_directories(catch2\n    INTERFACE\n        catch2\n)\n\nif (&lt;some_condition_to_detect_embedded_platform&gt;)\n    target_compile_definitions(catch2\n        INTERFACE\n            CATCH_CONFIG_NO_POSIX_SIGNALS\n            CATCH_CONFIG_DISABLE_EXCEPTIONS\n    )\nendif ()\n</code></pre> <p>Note the usage of <code>INTERFACE</code> keyword. When <code>add_library()</code> contains the <code>INTERFACE</code> specifier, then it tells CMake, that this target doesn\u2019t produce any binary. In such a case it doesn\u2019t contain any source files.</p> <p>As mentioned before, all properties of the <code>INTERFACE</code> target also have to be marked as <code>INTERFACE</code>. This is understandable, because header-only libraries don\u2019t have any private implementation. Everything is always accessible to the client. If this is still confusing for you then just remember, that <code>INTERFACE</code> target enforces <code>INTERFACE</code> properties. But later linking with such a target can be of any type:</p> <pre><code>add_library(myTestingModule\n    source.cpp\n)\n\ntarget_link_libraries(myTestingModule\n    PRIVATE\n        catch2\n)\n</code></pre>","tags":["CMake"]},{"location":"blog/modern-cmake-is-like-inheritance/#summary","title":"Summary","text":"<p>CMake provides a new target-oriented way of specifying various compiler options and other properties. Once you link with a target, you immediately inherit (obtain) its <code>INTERFACE</code> and <code>PUBLIC</code> properties and make it your own with the access level specified in the linking command. This mechanism resembles C++ inheritance, thus should be easy to understand.</p> <p>If you are using CMake 3.x and above, then use the this rule as your guide for creating targets:</p> <p>Library designed and built with Modern CMake should provide its clients with everything they need to compile and use it, without the need to check its internal implementation.</p> <p>If you find yourself checking what is the path to the missing include in some library then it means that you are doing something wrong:</p> <ul> <li>either CMake configuration of that library is bad,</li> <li>or you are trying to access files that are explicitly hidden from you.</li> </ul>","tags":["CMake"]},{"location":"blog/problems-with-external-dependencies-in-c/","title":"Problems with external dependencies in C++","text":"<p>Dependencies are usually the most problematic part of the build system (next to the toolchain setup) in programming projects. In C++ this is especially hard because there is no one standard build system. Many more problems arise if we target an embedded system. Other programming languages resolve this issue by having one built-in package manager (usually accompanied by an integrated build system). In C++ we lack both of them, so our lives are much harder. Fortunately, there are a few third-party package managers which get more and more popular. In this series of posts, I will try to convince you to start using one of them \u2013 Conan.</p> In this series <ol> <li>Problems with external dependencies in C++</li> <li>Introduction to Conan package manager</li> </ol>","tags":["Conan"]},{"location":"blog/problems-with-external-dependencies-in-c/#why-do-we-need-a-package-manager-for-c","title":"Why do we need a package manager for C++","text":"<p>Before I introduce you to Conan in the next article, let\u2019s try now to define all the problems, that our package manager should be able to solve, in order to call it \u201cgeneric\u201d and \u201cusable\u201d. Of course, let\u2019s not expect that it will be a remedy for everything out of the box. We are looking for a tool, that will provide us with simple mechanisms that can directly or indirectly address the day-to-day problems that most of us encounter in projects. Here is a list of immediate issues/concerns that one should look into before making any decision about dependencies or package managers:</p> <ol> <li>getting the sources of the external libraries \u2013 how are we actually going to get the sources of the libraries?    Should we hardcode the URLs in our build system? Or maybe we should incorporate them into our repository? The former    makes multiple assumptions about the location, format and availability of the library. The latter option increases    our codebase, makes it harder to update and de facto turns us into local maintainers of that dependency.</li> <li>managing versions of the dependencies \u2013 in some cases we need to use a very specific version of the external    library. For example, we want to use some hot new feature, which is available only in the latest release. How should    we specify that? How should we manage dependencies of the third-party code which could change from version to    version? This point is closely related to the next one.</li> <li>satisfying dependencies of the libraries \u2013 it is quite natural, that our dependencies will have their own    dependencies. In such a case we would have to meet some preconditions, possibly by installing another set of    libraries (which could again have dependencies). It is quite easy to get yourself into a loop of relations between    libraries that is hard to manage or even fully satisfy.</li> <li>dynamically selecting the required dependencies \u2013 sometimes we can build many applications from the same    codebase. It is usually the case when we have a lot of common code. In such situation it may happen, that one    application is using some part of the common code, but the second application is using another one. Both modules can    have different dependencies and we want to build only what is currently required. We would like to be able to    dynamically say that dependency A should be installed only while we compile \u201cthis\u201d directory and dependency B while    we compile \u201cthat\u201d directory. I have seen projects, in which such selection is not trivial and manually managing it    can generate a lot of boilerplate code in the build system.</li> <li>(cross-)compiling the libraries \u2013 if we are working on a PC application, then we don\u2019t have to worry about the    type of processor that will run our program or other platform-specific things. In the case of embedded systems, we    must cross-compile every source file that will be used. We need to be able to specify the concrete toolchain and all    required compilation flags.</li> <li>customizing/patching dependencies \u2013 in many cases we need to modify a little the external library. For example,    some of them are not ready for cross-compilation and their build system has to be adjusted. In other cases, it turns    out, that our dependency is not compiling with our compilation flags (e.g. we have -Werror flag enabled and the    library is producing warnings or we are using clang compiler and the external code is using GCC-specific extensions).    The most irritating thing (at least for me) is when I find a perfect library that solves my problem, but it is using    a different build system than mine. In such situations, we need to adapt it to our needs.</li> <li>installing compiled dependencies in the host system \u2013 once we compile the external libraries, they have to be    installed somewhere. There are two options: we either put them in our build directory or install them in the host    system. The first one will require us to build dependencies every time a build directory is cleaned. The second one    solves this problem, but installed binaries may be in conflict with the already installed packages in our operating    system. Also, we may want to distinguish libraries compiled with a different set of flags.</li> <li>making the whole process easy to use \u2013 it is important to keep it simple for the average programmer to build and    install the dependencies. If things get complicated, then our coworkers will be frustrated. They will either try to    remove the dependency and replace it with a self-written code (bad) or will modify a carefully designed build system    in their own way and thus may complicate it even more (equally bad).</li> <li>(optional) handling compilation on the target platform \u2013 in some rare (or not) cases we need to support building    the project on the target. In my experience, I had to do this in order to correctly calculate code coverage. Maybe I    did something wrong, nevertheless, the situation forced me to run a compilation on the board. It was quite    problematic because my target already had all the dependencies installed in the system. As a consequence build system    and package manager had to take that into an account and rather use those libraries than compiling them from scratch.</li> </ol> <p>These are only a few of problems that may arise in our projects. Also, once we choose one mechanism for managing the dependecies, then it will be hard to change it in the future. This is why this topic is important and requires a deep thought.</p>","tags":["Conan"]},{"location":"blog/problems-with-external-dependencies-in-c/#possible-solutions-for-external-dependencies","title":"Possible solutions for external dependencies","text":"<p>There are a few solutions for managing the dependencies on the market nowadays. It is good to know them at least briefly. Below you can find a list of the most popular that I know of:</p> <p>Note</p> <p>This list not complete and extremely brief. I have no experience with package managers other than Conan or CMake <code>fetch_content</code>. Consult the official docs to get more information and have a better overview on available solutions.</p>","tags":["Conan"]},{"location":"blog/problems-with-external-dependencies-in-c/#cmake-fetch_content","title":"CMake <code>fetch_content</code>","text":"<ul> <li>Official documentation: cmake.org.</li> <li>This mechanism is part of the CMake build system (read this article to learn more) and doesn\u2019t require   anything additional to be installed. It is very primitive but quite effective if your dependency is also using CMake   (non-CMake projects can also be built, but it is less flexible that way).</li> <li>It allows you to specify a repository or file to be downloaded. Additionally, you can select a branch or tag in case   of the git VCS.</li> <li>Dependencies are downloaded into the current build directory and can be built just like any other code in your   project. To be precise, you need to explicitly include them with a <code>add_subdirectory</code> command in <code>CMakeLists.txt</code>   file. This allows you to build third-party code with the exact same compilation configuration (toolchain and   compilation flags) as your codebase. On the other side, CMake treats that code as if it was your code which sometimes   can cause problems (warnings treated as errors or conflicting CMake variables).</li> </ul>","tags":["Conan"]},{"location":"blog/problems-with-external-dependencies-in-c/#hunter","title":"Hunter","text":"<ul> <li>Official documentation: hunter.readthedocs.io.</li> <li>Third-party package manager, which is fully dependant on CMake. In fact, Hunter is simply a set of CMake modules that   manage given dependencies internally via <code>ExternalProject_Add</code> command.</li> <li>Hunter provides a customization point for building the libraries via the native CMake toolchain file mechanism. This   is quite clean and nice, but it can be problematic if someone is not using toolchain files or don\u2019t know how they work   (read this article to learn more about toolchain files).</li> <li>Using libraries built by Hunter is done in a typical CMake way: after adding some initialization code in the   beginning, you can call <code>find_package</code> to locate the demanded library and use <code>target_link_libraries</code> to link against   it.</li> <li>Packages that are supported by Hunter have to be added to the official package repository and usually a bit modified   in order to cooperate with the package manager. Such libraries are said to be \u201cHunterized\u201d. It seems, that there is no   easy way to add custom packages other than through the official package list. This is quite limiting because it   usually takes time to analyze, review and accept such changes by the package manager maintainers. Also, this is   dangerous to depend on the work and time of people outside your organization or project. Sometimes you need to make   changes very quickly (e.g. hotfix for the client) and you can\u2019t have delays on the third-party people side.</li> <li>Hunter makes it easy to manage multiple versions and configurations of the same dependency because each build   directory is stored separately in a specified root path. Thus it is possible to reuse prebuilt packages in all your   projects.</li> </ul>","tags":["Conan"]},{"location":"blog/problems-with-external-dependencies-in-c/#vcpkg","title":"vcpkg","text":"<ul> <li>Official documentation: vcpkg.readthedocs.io.</li> <li>Like Hunter, this is a package manager but managed by Microsoft. In order to install it, you have to clone its   repository and compile it for your local machine. It runs on all major platforms and fully supports CMake by default.</li> <li>Installing dependencies is done in a separate step from the command line: <code>vcpkg install &lt;package_name&gt;</code>. All files   and binaries are stored in a user-specified root directory, so packages are not conflicting with the host system.   However, it is not clear to me, how is vcpkg actually building the code: is it assuming that we are targeting a local   machine? How should we specify the compiler and compilation flags that should be used during compilation? If you know,   then please share it in the comments.</li> <li>A major drawback (at least for me) is the fact, that in order to use dependencies provided by the vcpkg you need to   use a special toolchain file from the vcpkg root directory. I don\u2019t have any experience with this package manager   except for some tutorials and a few personal trials, but it seems that you can\u2019t explicitly use your own toolchain   file in the project. If that is true then this eliminates vcpkg for my projects.</li> <li>Incorporating dependencies managed by vcpkg is done exactly like in Hunter: <code>find_package</code> + <code>target_link_libraries</code> \u2013   clean and easy.</li> <li>Only libraries that are registered in the official vcpkg package repository can be used. This has a very similar   limitation as with Hunter: we depend on other people\u2019s time and will. However, since Microsoft is behind this, then we   can expect better support than usual. Also, it is said that Microsoft is running regular CI jobs with all registered   packages in order to maintain quality and quickly detect problems.</li> </ul>","tags":["Conan"]},{"location":"blog/problems-with-external-dependencies-in-c/#conan","title":"Conan","text":"<ul> <li>Official documentation: docs.conan.io.</li> <li>A package manager written in Python. Available in multiple forms, but the most recommended one is through a pip   package.</li> <li>It has very good integration with multiple build systems like CMake, Makefiles and Visual Studio. In the case of the   first one, packages can be used via specially created Conan targets for each library or with a well-known   <code>find_package</code> command.</li> <li>Dependencies can be built and installed both from the command line or directly from the CMake. The first method   expects a simple config file similar to pip\u2019s <code>requirements.txt</code> file.</li> <li>All binaries are stored in a per-user cache, allowing to reuse the prebuilt packages across compilations and projects.</li> <li>Conan is able to build external dependencies that use the most popular build systems like CMake, Autotools   (Makefiles), Visual Studio and Meson. Of course, it is up to the maintainer of the Conan package config (aka recipe)   to correctly set up the build process, but Conan provides all the required tools to do that. Regardless of which build   framework is used, we can quite easily reason about building the library and then (separately) reason about making a   binding out of its binary output. This is possible because for Conan these are two not related phases. In other words,   it is possible to use the external library which is using Makefiles and generate an integration layer to your CMake   project.</li> <li>Packages have to be registered in a package repository, but fortunately, you can freely add your own and register them   in a self-hosted or public repository for binaries. This is a huge benefit because you can maintain your own package   repository with your own package recipes even on your company\u2019s server. Usually, you don\u2019t need to keep a local copy   of the third-party code, because Conan packages should only tell the package manager where to get the sources and how   to build them. Additionally, you can apply custom patches in between.</li> </ul>","tags":["Conan"]},{"location":"blog/problems-with-external-dependencies-in-c/#yocto-sdk","title":"Yocto SDK","text":"<ul> <li>Official documentation yoctoproject.org.</li> <li>This mechanism is different from the previous ones, but since I\u2019m actively using it recently I decided to mention it   here. It comes from the Yocto Project, which is a Linux distribution generator/builder. It allows you to specify what   should be included in your final Linux image and for which platform it should be cross-compiled. Yocto SDK is a set of   libraries + toolchain that can be used by application developers to cross-compile applications that should be deployed   later into that image. It creates for you the same environment, that your application will meet on the target   (libraries, tools, etc).</li> <li>SDK installation is done with a Bash script and it simply installs all the packages and tools that your app can link   against in the development environment. It also provides you with a complete cross-toolchain and sets up all required   environment variables. If your build system is relying on the <code>CC</code> and <code>CXX</code> variables to detect the compiler, then   you won\u2019t have to change anything.</li> <li>Integration with CMake is done with a specially generated toolchain file.</li> <li>SDK is usually generated in one of the build stages of the Yocto image. Adding new packages or changing their versions   is done by simply running the Yocto build again and reinstalling the SDK. As a consequence, your build environment   will always reflect the newest target image, that will host your application.</li> </ul>","tags":["Conan"]},{"location":"blog/problems-with-external-dependencies-in-c/#summary","title":"Summary","text":"<p>Each of these package managers has advantages and disadvantages. It really depends on your needs and resources. I personally have the biggest (but still relatively small) experience with Conan. It served me well for the last year and allowed me to incorporate external libraries into my projects, which would never be possible otherwise within given time constraints. Mainly because of their lack of support for cross-compilation. With Conan, it was quite easy to adjust them without making a local copy of the package. In the next articles, I will try to present how Conan works and what are its main advantages and how I use it in my CMake projects.</p>","tags":["Conan"]},{"location":"blog/variadic-functions--part-1-va_args-and-friends/","title":"Variadic functions \u2013 Part 1: <code>va_args</code> and friends","text":"<p>From time to time there is a need to write a function, that accepts an unspecified number of arguments. In C++ we have multiple ways of handling this depending on the context, use case and available language features. But the oldest and still most commonly used mechanism is the <code>va_arg</code>.</p> <p>You think nothing can surprise you? Let\u2019s bet.</p> In this series <ol> <li>Variadic functions \u2013 Part 1: va_args and friends</li> <li>Variadic functions \u2013 Part 2: C++11 variadic templates</li> <li>Variadic functions \u2013 Part 3: techniques of variadic templates</li> </ol>"},{"location":"blog/variadic-functions--part-1-va_args-and-friends/#va_arg-in-a-nutshell","title":"<code>va_arg</code> in a nutshell","text":"<p>Variadic function by definition takes a variable number of arguments. C and C++ provide a builtin way of expressing this in the argument list by using the ellipsis (<code>...</code>).</p> <p>Here is a function, that accepts two doubles and an unknown number of other arguments:</p> <pre><code>void compute(double a, double b, int count...)\n{\n    // Some operations...\n}\n</code></pre> <p>In the example we provide an additional information to the function \u2013 number of arguments that are hidden behind the trailing ellipsis. It looks like an optional hint to the function implementer, but in fact it is required by the C standard (C++ refers to C on this matter).</p> <p>Valid usage of <code>va_args</code> in C++ requires that:</p> <ul> <li>ellipsis must be last in the arguments list,</li> <li>last named parameter must define the count of variadic arguments,</li> <li>ellipsis can be preceded with an optional comma.</li> </ul> <p>Note</p> <p>C++ added an optional comma before the ellipsis to be compatible with C, where comma is obligatory.</p> <p>The standard library provides the  header with the following tools to work with the variadic arguments: <ul> <li><code>va_list</code> \u2013 helper type, that represents the variadic arguments list,</li> <li><code>va_start()</code> \u2013 initializes the <code>va_list</code> object in the current function,</li> <li><code>va_arg()</code> \u2013 extracts the next variadic argument,</li> <li><code>va_end()</code> \u2013 destroys the <code>va_list</code> object.</li> </ul> <p>The <code>va_list</code> type is implementation-defined and shouldn\u2019t be accessed directly. Its only purpose is to be used by the <code>va_*</code> macros family. The exact definition is usually provided by a builtin in the compiler and depends on the CPU and ABI (Application Binary Interface), which defines the calling convention (how arguments are passed from caller to the callee).</p> <p>For the curious</p> <p>Here are two example implementations found in the wild:</p> <p>\u2013 pointer to the stack (old Apple kernel implementation), \u2013 custom structure (System V AMD64 ABI specification).</p> <p><code>va_start()</code> macro initializes the <code>va_list</code> object using the number of expected variadic arguments. It is required to be called before any access to the variable arguments.</p> <p>The central role of the happy variadic family plays the <code>va_arg()</code> function macro. It is used to extract the argument of the given type from the current parameters list\u2019s head. We can visualize this process as casting the varargs pointer to the demanded type and moving it later by the size of the type.</p> <p><code>va_end()</code> macro destroys the helper object and has to be called in the same function as the corresponding <code>va_start()</code> call. It also must be called before any redefinition of there <code>va_list</code> type.</p> <p>Here is a complete example of a valid variadic function usage:</p> <pre><code>#include &lt;cstdarg&gt;\n\nint sum(int count, ...)\n{\n    int sum = 0;\n    va_list args;\n    va_start(args, count);\n\n    for (int i = 0; i &lt; count; ++i) {\n        int num = va_arg(args, int);\n        sum += num;\n    }\n\n    va_end(args);\n    return sum;\n}\n\nint main()\n{\n    return sum(4, 6, 7, 8, 9);\n}\n</code></pre> <p>Feel free to play and inspect this example on different platforms and compilers using Compiler Explorer.</p>"},{"location":"blog/variadic-functions--part-1-va_args-and-friends/#default-type-promotion","title":"Default type promotion","text":"<p>There is one interesting process that happens behind the scenes when using variadic arguments. The C standard states, that all unnamed arguments that are part of the variadic list are subject to the default type promotion. It means, that all variadic arguments are converted in the following way:</p> <ul> <li><code>std::nullptr_t</code> is converted to <code>void*</code>,</li> <li><code>float</code> arguments are converted to <code>double</code>,</li> <li><code>bool</code>, <code>char</code>, <code>short</code> and unscoped enumerations are converted to <code>int</code> or wider integer types.</li> </ul> <p>It is really important to remember, because it\u2019s not obvious. For example: if you pass <code>char</code> or <code>bool</code>, you still have to extract an <code>int</code>! Here is an example:</p> <pre><code>#include &lt;cstdarg&gt;\n\nvoid func2(int count\u2026)\n{\n    va_list ap; \n    va_start(ap, count);\n\n    auto b = static_cast&lt;bool&gt;(va_arg(ap, int));\n\n    va_end(ap);\n}\n\nvoid func()\n{\n    func2(1, true);\n}\n</code></pre>"},{"location":"blog/variadic-functions--part-1-va_args-and-friends/#sources-of-the-undefined-behavior","title":"Sources of the undefined behavior","text":"<p>Variadic arguments from <code>&lt;cstdarg&gt;</code> is a simple mechanism. However, it is easy to introduce some serious bugs to your program if you are not careful. Here is the list of rules to obey to avoid the UB while using <code>va_args</code>:</p> <ul> <li>Do not pass parameters count to <code>va_args</code> by reference.</li> <li>Always define variadic arguments count to be the last named parameter (right before the ellipsis).</li> <li>Do not use <code>va_list</code> before calling <code>va_start()</code> first.</li> <li>Do not use <code>va_list</code> after calling <code>va_end()</code>.</li> <li>Do not call <code>va_end()</code> in a different function that corresponding <code>va_start()</code>.</li> <li>Do not call <code>va_start()</code> on the previously initialized object without calling <code>va_end()</code> first.</li> <li>Do not call <code>va_arg()</code> more times than the actual number of arguments.</li> <li>Do not call <code>va_arg()</code> with the incorrect type.</li> </ul> <p>Remember</p> <p>It doesn\u2019t matter that you checked the violated rule and it works. It is just luck.</p>"},{"location":"blog/variadic-functions--part-1-va_args-and-friends/#static-analysis","title":"Static analysis","text":"<p>Variadic arguments are operating directly on the stack arguments, so no wonder why this API has so many constraints. But fear not! Your favorite compiler is here to help. It turns out, that both GCC and Clang have the default <code>-Wvarargs</code> warning enabled. They can check if you are providing the number of hidden arguments in the arguments list, if this parameter is right before the ellipsis and if by any chance you are passing a reference to the <code>va_start</code> macro.</p> <p>It is also checked by the popular static analyzers: clang-tidy and Parasoft C/C++ Static Analyzer.</p>"},{"location":"blog/variadic-functions--part-1-va_args-and-friends/#alternatives-to-va_arg","title":"Alternatives to <code>va_arg</code>","text":"<p>Presented <code>va_arg</code> mechanism from <code>&lt;cstdarg&gt;</code> is no the only way to implement the variadic functions. In specific use cases you can use other alternatives:</p> <p>function overloads \u2013 handy in a situation, where there are not that many combinations of the arguments and they are handled in a different way, function accepting a container of arguments (e.g. <code>std::vector</code>, <code>std::array</code> or <code>std::initializer_list</code>) \u2013 this approach can be used only if arguments share a common type. Of course, we can use the type erasure technique (<code>std::any</code>, <code>std::variant</code>), but it will add another layer to the solution making it harder to understand, variadic templates \u2013 which will be discussed in the second part of this article.</p>"},{"location":"blog/variadic-functions--part-1-va_args-and-friends/#conclusion","title":"Conclusion","text":"<p>Variadic functions with <code>va_arg</code> support have both pros and cons. They give you freedom and flexibility in handling particular tasks. But also they require a strict contract between the caller and the callee, which is hard to maintain. They are not type safe (only rely on the contract) and can lead to undefined behavior. There are multiple recommendations to avoid it (e.g. C++ Core Guidelines and SEI CERT C++ Coding Standard), which I agree with. However I you have no other option or you have to use an API that already takes the <code>va_args</code> \u2013 be careful and RTFM!</p>"},{"location":"blog/variadic-functions--part-2-c11-variadic-templates/","title":"Variadic functions \u2013 Part 2: C++11 variadic templates","text":"<p>In the previous article, we have seen an old C-style way of defining the variadic functions \u2013 via va_args macros. We have also learned their weaknesses: runtime cost, strict contract between caller and the callee and lack of type safety. Not to mention the problem of default type promotion!</p> <p>Fortunately, standards committee thought this through and equipped us in C++11 with variadic templates. Let\u2019s check them out.</p> In this series <ol> <li>Variadic functions \u2013 Part 1: va_args and friends</li> <li>Variadic functions \u2013 Part 2: C++11 variadic templates</li> <li>Variadic functions \u2013 Part 3: techniques of variadic templates</li> </ol>"},{"location":"blog/variadic-functions--part-2-c11-variadic-templates/#syntax-definitions","title":"Syntax &amp; definitions","text":"<p>First, let\u2019s start with a basic hello world of variadic templates:</p> <pre><code>#include &lt;iostream&gt;\n\ntemplate &lt;typename T&gt;\nT sum(T arg)\n{\n    return arg;\n}\n\ntemplate &lt;typename T, typename... Args&gt;\nT sum(T first, Args... args)\n{\n    return first + sum(args...);\n}\n\nint main(int argc, char* argv[])\n{\n    auto result = sum(1, 3, 5, 7);\n    std::cout &lt;&lt; result &lt;&lt; '\\n';\n    return 0;\n}\n</code></pre> <p>The output of this program is of course:</p> <pre><code>16\n</code></pre> <p>We have two template functions: first taking single parameter and second taking\u2026 exactly what? There are three syntax changes compared to the regular templates, that make the second function \u201cvariadic\u201d:</p> <pre><code>typename... Args\n</code></pre> <p>which is called template parameter pack, then there is:</p> <pre><code>Args... args\n</code></pre> <p>which is called function parameter pack and finally:</p> <pre><code>args...\n</code></pre> <p>which is called pack expansion.</p> <p>Template parameter pack defines a list of unspecified (possibly different) types that will be used to instantiate this template function. Function parameter pack is using the same list of types to create the function argument list. The notion of ellipsis in this context is similar to that known from the <code>va_args</code> functions. The difference and real strength of variadic templates comes in the argument expansion.</p> <p>Note</p> <p>The reason for having the first argument explicitly stated and having an overload will be explained later.</p>"},{"location":"blog/variadic-functions--part-2-c11-variadic-templates/#parameter-pack-expansion","title":"Parameter pack expansion","text":"<p>When compiler sees an ellipsis in the template it automatically expands the expression that uses it into a comma-separated list according to the context. In our example we have three different places where <code>...</code> is used. Let\u2019s see in pseudo-code how compiler might unpack them:</p> CONTEXT EXPANSION Template parameter pack <code>typename... Args</code> -&gt; <code>Arg1, Arg2, Arg3, ..., ArgN</code> Function parameter pack <code>Args... args</code> -&gt; <code>Arg1 arg1, Arg2 arg3, Arg3 arg3, ..., ArgN argN</code> Pack expansion <code>sum(args...)</code> -&gt; <code>sum(arg1, arg2, arg3, ..., argN)</code> <p>Note</p> <p>And now the best of it:</p> <ol> <li>All happens at compile time (no runtime cost and allows optimizations)!</li> <li>All types are preserved (ensuring type safety)!</li> <li>We still haven\u2019t used any specific type (no strict contract between caller and callee)!</li> </ol> <p>That makes variadic templates superior to <code>va_arg</code> in almost every aspect. However, the drawback of this solution is a different approach to implementing the function. This requires a bit of a mindset switch and time to get used to it.</p>"},{"location":"blog/variadic-functions--part-2-c11-variadic-templates/#example-explanation","title":"Example explanation","text":"<p>And now is the time to explain the implementation of our adding function. The idea is to use the recursion, extract the first parameter from the argument pack in each call and pass the rest to the next iteration. We add the first unpacked argument to the result of the remaining recursion. This process is repeated until the argument pack has only one element. Then we call the function overload that expects a single template argument. This overload prevents further recursion and is often called the \u201cbase case\u201d. You can find the analogy to the popular <code>Factorial&lt;N&gt;</code> example. Finally this call:</p> <pre><code>auto result = sum(1, 3, 5, 7);\n</code></pre> <p>might result in the following expansion steps made by the compiler:</p> <pre><code>auto result = sum(1, 3, 5, 7);    // 1) variadic case\nauto result = 1 + sum(3, 5, 7);   // 2) variadic case\nauto result = 1 + 3 + sum(5, 7);  // 3) variadic case\nauto result = 1 + 3 + 5 + sum(7); // 4) base case\nauto result = 1 + 3 + 5 + 7;      // 5) full expansion\n</code></pre>"},{"location":"blog/variadic-functions--part-2-c11-variadic-templates/#implementation-notes","title":"Implementation notes","text":"<p>I said earlier that variadic templates eliminate the strict contract between the called function and its client. This is true to some extent. In C-style variadic functions strict contract means, that function author must specify the exact list of acceptable types along with its order (either in docs or by other parameter like format string). Any difference between implementation and the call-side might lead to undefined behavior.</p> <p>In case of variadic templates, then only requirement for the caller is to provide the types that support operations used on them within the template body. In our case it is addition. So in other words, we require that for every pair of types from the template parameter pack there is a well-defined <code>operator+()</code>.</p> <p>Note</p> <p>One possible way of ensuring this is to use the concepts from C++20 aka named requirements.</p>"},{"location":"blog/variadic-functions--part-2-c11-variadic-templates/#instantiating-template-with-list-of-different-types","title":"Instantiating template with list of different types","text":"<p>Arguments don\u2019t have to be all of the same type in the template parameter list. We only require the <code>+</code> operation to be valid for every pair. So the following call is perfectly valid:</p> <pre><code>sum(true, 0, 4.6, 3.14f);\n</code></pre> <p>But the result is quite unexpected:</p> <pre><code>1\n</code></pre> <p>The problem lies in the result type of our template. Lets add some logging to better understand the situation:</p> <pre><code>#include &lt;iostream&gt;\n#include &lt;typeinfo&gt;\n\ntemplate &lt;typename T&gt;\nT sum(T arg)\n{\n    return arg;\n}\n\ntemplate &lt;typename T, typename... Args&gt;\nT sum(T first, Args... args)\n{\n    return first + sum(args...);\n}\n\nint main(int argc, char* argv[])\n{\n    auto result = sum(true, 0, 4.6, 3.14f);\n    std::cout &lt;&lt; typeid(result).name() &lt;&lt; \": \" &lt;&lt; result &lt;&lt; '\\n';\n    return 0;\n}\n</code></pre> <p>We have added type name of the result to the output. Now the program prints:</p> <pre><code>b: 1\n</code></pre> <p>This means, that the result variable is <code>bool</code>. The reason for that, is because our first template instantiation has <code>bool</code> as the first argument. At that moment the return value is also <code>bool</code>. And this is the type used in type deduction for the <code>result</code> variable. The recursion works as expected, but the return values are casted to <code>bool</code> which can hold only <code>0</code> and <code>1</code> values.</p> <p>In C++14 we can ask the compiler to deduce the correct type for us by changing the return types to <code>auto</code>.</p> <pre><code>#include &lt;iostream&gt;\n#include &lt;typeinfo&gt;\n\ntemplate &lt;typename T&gt;\nauto sum(T arg)\n{\n    return arg;\n}\n\ntemplate &lt;typename T, typename... Args&gt;\nauto sum(T first, Args... args)\n{\n    return first + sum(args...);\n}\n\nint main(int argc, char* argv[])\n{\n    auto result = sum(true, 0, 4.6, 3.14f);\n    std::cout &lt;&lt; typeid(result).name() &lt;&lt; \": \" &lt;&lt; result &lt;&lt; '\\n';\n    return 0;\n}\n</code></pre> <p>Now the result is as expected:</p> <pre><code>d: 8.74\n</code></pre>"},{"location":"blog/variadic-functions--part-2-c11-variadic-templates/#going-further","title":"Going further","text":"<p>Recursion is not the only technique used with variadic templates. Our toolbox contains also:</p> <ul> <li>expression expansion,</li> <li>creative trick (read \u201chack\u201d) with <code>std::initializer_list</code>,</li> <li>C++17 fold expressions,</li> <li><code>sizeof...</code> operator.</li> </ul> <p>But all of that will be covered in the next article. Stay tuned.</p>"},{"location":"blog/variadic-functions--part-3-techniques-of-variadic-templates/","title":"Variadic functions \u2013 Part 3: Techniques of variadic templates","text":"<p>In the previous article I have shown you how variadic templates can be a type-safe alternative to va_arg macros in designing the variadic functions. Today I want to show you a few techniques of variadic templates, that can be found in many codebases.</p> In this series <ol> <li>Variadic functions \u2013 Part 1: va_args and friends</li> <li>Variadic functions \u2013 Part 2: C++11 variadic templates</li> <li>Variadic functions \u2013 Part 3: techniques of variadic templates</li> </ol>"},{"location":"blog/variadic-functions--part-3-techniques-of-variadic-templates/#recursive-variadic-templates","title":"Recursive variadic templates","text":"<p>This technique has been already covered in the previous post, but let\u2019s keep it anyway for completeness.</p> <pre><code>#include &lt;iostream&gt;\n\nvoid showRecursive() {}\n\ntemplate &lt;typename T, typename... Args&gt;\nvoid showRecursive(T first, Args... args)\n{\n    std::cout &lt;&lt; \"showRecursive: \" &lt;&lt; first &lt;&lt; '\\n';\n    showRecursive(args...);\n}\n\nint main()\n{\n    showRecursive(1, 2, 3, 4);\n    return 0;\n}\n</code></pre> <p>Here we have a slightly different example with only one template. <code>showRecursive</code> is a function that prints its arguments, each in a separate line. As we already know, we have to unpack the first argument from the parameter pack by explicitly declaring it in the function parameter list. Thus each instantiation of the template will assign its leading argument to the <code>first</code> variable and leave the rest as the remaining pack.</p> <p>In order to enable the recursion, we call the same function with the expansion of the remaining parameter pack. In every iteration the argument pack will be shorter by one element. Finally, in the last round <code>showRecursive</code> will be called with an empty pack. To handle this base case we can use a simple overload that takes no arguments (this is why it doesn\u2019t need to be a template).</p> <p>The expected result is of course:</p> <pre><code>showRecursive: 1\nshowRecursive: 2\nshowRecursive: 3\nshowRecursive: 4\n</code></pre>"},{"location":"blog/variadic-functions--part-3-techniques-of-variadic-templates/#non-recursive-argument-evaluation-with-stdinitializer_list","title":"Non-recursive argument evaluation with <code>std::initializer_list</code>","text":"<p>Variadic templates don\u2019t have to be recursive. In fact this approach has several drawbacks:</p> <ul> <li>it makes the code less readable,</li> <li>it potentially generates more code,</li> <li>it requires a correct base case (stop condition) implementation.</li> </ul> <p>Our preferred way is to iterate over all arguments and pass them one by one to some single argument function (like in the <code>for</code> loop) and ultimately get something like this:</p> <pre><code>func(arg1), func(arg2), ..., func(argN);\n</code></pre> <p>C++ doesn\u2019t have an explicit way of achieving that. Fortunately, there is an implicit method, that involves usage of the infamous <code>std::initializer_list</code>. Let\u2019s see:</p> <pre><code>#include &lt;initializer_list&gt;\n#include &lt;iostream&gt;\n\ntemplate &lt;typename T&gt;\nvoid showImpl(T arg)\n{\n    std::cout &lt;&lt; \"showImpl: \" &lt;&lt; arg &lt;&lt; '\\n';\n}\n\ntemplate &lt;typename... Args&gt;\nvoid showNonRecursive(Args... args)\n{\n    std::initializer_list&lt;int&gt;{(showImpl(args), 0)...};\n}\n\nint main()\n{\n    showNonRecursive(1, 2, 3, 4);\n    return 0;\n}\n</code></pre> <p>The first notable difference in this example is lack of the first unpacked argument. We are not basing out implementation on the recursion, so it is not needed. Instead the <code>showNonRecursive</code> is creating an object of <code>std::initializer_list&lt;int&gt;</code>. The argument of its constructor is particularly interesting:</p> <pre><code>(showImpl(args), 0)...\n</code></pre> <p>This is a pack expansion, but used on the comma operator expression. Arguments of the comma operator are always evaluated left-to-right, but the result is equal to the last argument. So in our case the first argument (which is a call to <code>showImpl</code>) is evaluated with the first unpacked value from the parameter pack and <code>0</code> is returned as a result of the comma operator. Then the second unpacked argument is passed to the constructor of the <code>std::initializer_list</code> in the same manner. And again, the result of the operator is <code>0</code>. In pseudo-code this operation may be executed like this:</p> <pre><code>std::initializer_list&lt;int&gt;{(showImpl(arg1), 0), (showImpl(arg2), 0), ..., (showImpl(argN), 0)};\n</code></pre> <p>If we leave only the side effects (results of the expressions), then it may look like this:</p> <pre><code>std::initializer_list&lt;int&gt;{0, 0, ..., 0};\n</code></pre> <p>This explains, why we used <code>int</code> as a template argument in the <code>std::initializer_list</code>. In the end, the created object is not assigned to any variable, so compiler is free to optimize it away (however I didn\u2019t confirmed that it actually does that).</p> <p>To sum up, we are creating a fake object only to use the fact, that initializer lists are guaranteed to be evaluated in order. Since we can\u2019t create an initializer list out of the parameter pack (which could potentially have multiple types), we use a list of 0s preceded with a demanded function. The glue that makes it work is the comma operator, which evaluates both <code>showImpl</code> and 0, leaving 0 as the side effect.</p> <p>Note</p> <p>For people used to C++98 style or earlier this seems like a cool trick. For newcomers it could look like a hack in a language where everything is difficult. I agree \u2013 this is a HACK that exploits side effects of the comma operator. But let\u2019s not forget, that templates were never intended to be used this way.</p>"},{"location":"blog/variadic-functions--part-3-techniques-of-variadic-templates/#expression-expansion","title":"Expression expansion","text":"<p>Parameter expansion can be used in many ways and in different code contexts. In the previous example we have seen that applying a function on the parameter pack in fact calls this function for every element of the pack. The same can be done with the single argument operators like incrementing, casting or extracting an address. Below you can find a list of most interesting use cases for the expression expansion:</p> CONTEXT EXPANSION Function arguments <code>f(sizeof(args)...)</code> -&gt; <code>f(sizeof(arg1), sizeof(arg2), ..., sizeof(argN)</code> Template arguments <code>print&lt;Param1, Args..., Param2&gt;</code> -&gt; <code>print&lt;Param1, Arg1, Arg2, ..., ArgN, Param2&gt;</code> Inheritance list <code>template class Derived : public Bases... {}</code> -&gt; <code>template &lt;typename Base1, typename Base2, ..., typename BaseN&gt; class Derived : public Base1, public Base2, ..., public BaseN {}</code> Lambda capture list <code>[args...] {}</code> -&gt; <code>[arg1, arg2, ..., argN] {}</code>"},{"location":"blog/variadic-functions--part-3-techniques-of-variadic-templates/#c17-fold-expressions","title":"C++17 fold expressions","text":"<p>C++17 added even more operations, that can be used with the parameter pack. The main idea is to allow using two-argument operators. C++17 standard specifies 4 acceptable syntax variations for this:</p> <ol> <li>unary right fold (<code>pack op ...</code>),</li> <li>unary left fold (<code>... op pack</code>),</li> <li>binary right fold (<code>pack op ... op init</code>),</li> <li>binary left fold (<code>init op ... op pack</code>).</li> </ol> <p>So our summing example could be implemented using the unary right fold expression like this:</p> <pre><code>#include &lt;iostream&gt;\n\ntemplate &lt;typename... Args&gt;\nauto sum(Args... args)\n{\n    return (args + ...);\n}\n\nint main()\n{\n   std::cout &lt;&lt; sum(1, 2, 3, 4) &lt;&lt; '\\n';\n   return 0;\n}\n</code></pre> <p>On the other hand, the \u201cno separate line\u201d variation of the printing example could look like this using the binary left fold expression:</p> <pre><code>#include &lt;iostream&gt;\n\ntemplate &lt;typename... Args&gt;\nvoid foldPrint(Args... args)\n{\n    (std::cout &lt;&lt; \"foldPrint: \" &lt;&lt; ... &lt;&lt; args) &lt;&lt; '\\n';\n}\n\nint main()\n{\n    foldPrint(1, 2, 3, 4);\n    return 0;\n}\n</code></pre> <p>The result of the above code is:</p> <pre><code>foldPrint: 1234\n</code></pre> <p>Below you can find the list of all acceptable operators in the fold expressions:</p> <pre><code>+ \u2003\u2002- \u2003\u2002* \u2003\u2002/ \u2003\u2002% \u2003\u2002^ \u2003\u2002&amp; \u2003\u2002| \u2003\u2002&lt;&lt; \u2003\u2002&gt;&gt; \n+=\u2003\u2002-=\u2003\u2002*=\u2003\u2002/=\u2003\u2002%=\u2003\u2002^=\u2003\u2002&amp;=\u2003\u2002|=\u2003\u2002&lt;&lt;=\u2003\u2002&gt;&gt;=\u2003\u2002=\n==\u2003\u2002!=\u2003\u2002&lt; \u2003\u2002&gt; \u2003\u2002&lt;=\u2003\u2002&gt;=\u2003\u2002&amp;&amp;\u2003\u2002||\u2003\u2002,  \u2003\u2002.* \u2003\u2002-&gt;*\n</code></pre>"},{"location":"blog/variadic-functions--part-3-techniques-of-variadic-templates/#usage-of-the-sizeof-operator","title":"Usage of the <code>sizeof...</code> operator","text":"<p>When using variadic templates you may end up in a situation, where you would like to know how many arguments are actually passed. Let\u2019s say that you want to store them in a table. How big should it be? <code>sizeof...()</code> will tell you:</p> <pre><code>template &lt;typename... Args&gt;\nvoid func(Args... args)\n{\n    int argsTable[sizeof...(args)] = {args...}; \n    // Some other code.\n}\n</code></pre> <p>This technique is pretty straightforward. <code>sizeof...</code> operator called on the parameter pack will tell you the number of its elements at compile time. It can be used in any context, that requires the compile-time evaluation.</p>"},{"location":"blog/variadic-functions--part-3-techniques-of-variadic-templates/#perfect-forwarding-fyi","title":"Perfect forwarding (FYI)","text":"<p>There is one last very important technique called perfect forwarding. It is not limited only to variadic templates, but in my opinion it brings the most benefits in this context. Its only purpose is to pass (forward) function template arguments into another function preserving all their properties like qualifiers or value semantics (lvalue, rvalue etc\u2026). However, it requires more knowledge about universal/forward references (<code>&amp;&amp;</code>) and move semantics. It is also very rarely found in a typical business code, thus I will not cover its details here. Usually, it is exploited in the highly customizable libraries, where client can specify its own callback functions. STL would be an excellent example of this. Some of the most popular standard functions and classes that use perfect forwarding are:</p> <ul> <li><code>std::make_shared()</code> / <code>std::make_unique()</code>,</li> <li><code>std::thread</code>,</li> <li><code>std::function</code>,</li> <li><code>std::vector::emplace_back()</code>.</li> </ul> <p>Note</p> <p>Even Bjarne Stroustrup was joking (or not), that he can\u2019t write perfect forwarding with confidence without checking some details first. Not to mention how little percentage of programmers actually should care about understanding it. Herb Sutter mentions this in his \"Back to the Basics! Essentials of Modern C++ Style\" talk at CppCon 2014 (direct link to the quote).</p>"},{"location":"blog/variadic-functions--part-3-techniques-of-variadic-templates/#summary","title":"Summary","text":"<p>I hope that now you know what can be done with variadic templates and how to start using it. cppreference can be a useful resource when in doubt.</p>"},{"location":"blog/archive/2021/","title":"2021","text":""},{"location":"blog/archive/2020/","title":"2020","text":""},{"location":"blog/archive/2019/","title":"2019","text":""},{"location":"blog/archive/2018/","title":"2018","text":""},{"location":"blog/category/tools/","title":"Tools","text":""},{"location":"blog/category/c/","title":"C++","text":""},{"location":"blog/category/personal/","title":"Personal","text":""}]}